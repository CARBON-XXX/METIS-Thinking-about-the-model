"""
SEDAC V9.0 - è‡ªé€‚åº”è®­ç»ƒå™¨

ç›®æ ‡ï¼šé€€å‡ºç²¾åº¦ 95%+

æ ¸å¿ƒæ”¹è¿›ï¼š
1. å¤šä»»åŠ¡å­¦ä¹ ï¼šæ”¯æŒä¸åŒä»»åŠ¡ç±»å‹
2. è®¤çŸ¥è´Ÿè·é¢„æµ‹ï¼šè¿ç»­å€¼è€Œéç¦»æ•£æ ‡ç­¾
3. è‡ªé€‚åº”æŸå¤±æƒé‡ï¼šæ ¹æ®æ ·æœ¬éš¾åº¦åŠ¨æ€è°ƒæ•´
4. æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
5. å›°éš¾æ ·æœ¬æŒ–æ˜ï¼šé‡ç‚¹è®­ç»ƒè¾¹ç•Œæ ·æœ¬
"""

from __future__ import annotations
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import json
import logging
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

from sedac.v8.intuition_network import IntuitionNetwork, IntuitionConfig

logger = logging.getLogger(__name__)


@dataclass
class AdaptiveTrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # æ•°æ®
    data_path: str = "sedac_v9_augmented_data.json"
    val_split: float = 0.15
    
    # è®­ç»ƒ
    epochs: int = 100
    batch_size: int = 128
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    
    # å­¦ä¹ ç‡è°ƒåº¦
    warmup_epochs: int = 5
    min_lr: float = 1e-6
    
    # æŸå¤±æƒé‡ï¼ˆåˆå§‹å€¼ï¼Œä¼šè‡ªé€‚åº”è°ƒæ•´ï¼‰
    exit_weight: float = 1.0
    confidence_weight: float = 1.0
    cognitive_load_weight: float = 0.5
    
    # æ­£åˆ™åŒ–
    dropout: float = 0.2
    label_smoothing: float = 0.1
    
    # å›°éš¾æ ·æœ¬æŒ–æ˜
    hard_sample_ratio: float = 0.3
    
    # æ—©åœ
    patience: int = 15
    min_delta: float = 1e-4
    
    # ä¿å­˜
    save_dir: str = "checkpoints"
    
    # è®¾å¤‡
    device: str = "auto"


class AdaptiveDataset(Dataset):
    """
    è‡ªé€‚åº”æ•°æ®é›†
    
    æ”¯æŒï¼š
    - å¤šä»»åŠ¡ç±»å‹
    - è®¤çŸ¥è´Ÿè·æ ‡ç­¾
    - æ ·æœ¬æƒé‡ï¼ˆå›°éš¾æ ·æœ¬æŒ–æ˜ï¼‰
    """
    
    def __init__(
        self,
        samples: List[Dict],
        num_layers: int = 36,
        compute_weights: bool = True,
    ):
        self.samples = samples
        self.num_layers = num_layers
        
        # å±•å¹³æ•°æ®
        self.flat_data = []
        for sample in samples:
            features_per_layer = sample["features_per_layer"]
            is_correct = sample.get("is_correct", True)
            is_ood = sample.get("is_ood", False)
            optimal_exit = sample.get("optimal_exit_layer", num_layers)
            final_entropy = sample.get("final_entropy", 1.0)
            cognitive_load = sample.get("cognitive_load", 0.5)
            difficulty = sample.get("difficulty", 0.5)
            task_type = sample.get("task_type", "unknown")
            
            for layer_idx, features in enumerate(features_per_layer):
                # è®¡ç®—è¯¥å±‚æ˜¯å¦å¯ä»¥å®‰å…¨é€€å‡º
                can_exit = layer_idx >= optimal_exit and is_correct and not is_ood
                
                # è®¡ç®—è¯¥å±‚çš„è®¤çŸ¥è´Ÿè·ï¼ˆéšå±‚æ•°é€’å‡ï¼‰
                layer_progress = layer_idx / max(num_layers - 1, 1)
                layer_cognitive_load = cognitive_load * (1.0 - 0.5 * layer_progress)
                
                self.flat_data.append({
                    "features": features,
                    "layer_idx": layer_idx,
                    "is_correct": is_correct,
                    "is_ood": is_ood,
                    "optimal_exit_layer": optimal_exit,
                    "can_exit": can_exit,
                    "final_entropy": final_entropy,
                    "cognitive_load": layer_cognitive_load,
                    "difficulty": difficulty,
                    "task_type": task_type,
                })
        
        # è®¡ç®—æ ·æœ¬æƒé‡
        self.weights = None
        if compute_weights:
            self._compute_sample_weights()
        
        logger.info(f"Created dataset with {len(self.flat_data)} samples")
    
    def _compute_sample_weights(self):
        """
        è®¡ç®—æ ·æœ¬æƒé‡
        
        å›°éš¾æ ·æœ¬ï¼ˆè¾¹ç•Œæ ·æœ¬ï¼‰è·å¾—æ›´é«˜æƒé‡
        """
        weights = []
        for item in self.flat_data:
            # åŸºç¡€æƒé‡
            w = 1.0
            
            # å›°éš¾æ ·æœ¬åŠ æƒ
            difficulty = item["difficulty"]
            w *= 1.0 + difficulty  # éš¾åº¦è¶Šé«˜æƒé‡è¶Šå¤§
            
            # è¾¹ç•Œå±‚åŠ æƒï¼šæ¥è¿‘æœ€ä¼˜é€€å‡ºå±‚çš„æ ·æœ¬æ›´é‡è¦
            layer_idx = item["layer_idx"]
            optimal_exit = item["optimal_exit_layer"]
            distance_to_optimal = abs(layer_idx - optimal_exit)
            if distance_to_optimal < 5:
                w *= 2.0  # è¾¹ç•Œæ ·æœ¬åŠ å€æƒé‡
            
            # é”™è¯¯æ ·æœ¬åŠ æƒï¼ˆé˜²æ­¢å‡é˜³æ€§ï¼‰
            if not item["is_correct"]:
                w *= 1.5
            
            # OODæ ·æœ¬åŠ æƒ
            if item["is_ood"]:
                w *= 1.5
            
            weights.append(w)
        
        # å½’ä¸€åŒ–
        total = sum(weights)
        self.weights = [w / total * len(weights) for w in weights]
    
    def __len__(self) -> int:
        return len(self.flat_data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        item = self.flat_data[idx]
        
        return {
            "features": torch.tensor(item["features"], dtype=torch.float32),
            "layer_idx": torch.tensor(item["layer_idx"], dtype=torch.long),
            "is_correct": torch.tensor(float(item["is_correct"]), dtype=torch.float32),
            "is_ood": torch.tensor(float(item["is_ood"]), dtype=torch.float32),
            "can_exit": torch.tensor(float(item["can_exit"]), dtype=torch.float32),
            "optimal_exit_layer": torch.tensor(item["optimal_exit_layer"], dtype=torch.float32),
            "cognitive_load": torch.tensor(item["cognitive_load"], dtype=torch.float32),
            "difficulty": torch.tensor(item["difficulty"], dtype=torch.float32),
        }


class AdaptiveLoss(nn.Module):
    """
    è‡ªé€‚åº”å¤šä»»åŠ¡æŸå¤±
    
    ç»„ä»¶ï¼š
    1. é€€å‡ºå†³ç­–æŸå¤±ï¼šBCE with focal loss
    2. ç½®ä¿¡åº¦æ ¡å‡†æŸå¤±ï¼šECE-aware
    3. è®¤çŸ¥è´Ÿè·å›å½’æŸå¤±ï¼šHuber loss
    4. OODæ£€æµ‹æŸå¤±ï¼šBCE
    """
    
    def __init__(
        self,
        exit_weight: float = 1.0,
        confidence_weight: float = 1.0,
        cognitive_load_weight: float = 0.5,
        label_smoothing: float = 0.1,
        focal_gamma: float = 2.0,
    ):
        super().__init__()
        self.exit_weight = exit_weight
        self.confidence_weight = confidence_weight
        self.cognitive_load_weight = cognitive_load_weight
        self.label_smoothing = label_smoothing
        self.focal_gamma = focal_gamma
        
        # è‡ªé€‚åº”æƒé‡ï¼ˆä¼šåœ¨è®­ç»ƒä¸­è°ƒæ•´ï¼‰
        self.adaptive_weights = nn.Parameter(
            torch.ones(4), requires_grad=False
        )
    
    def focal_bce_loss(
        self,
        pred: torch.Tensor,
        target: torch.Tensor,
        gamma: float = 2.0,
    ) -> torch.Tensor:
        """
        Focal BCE Loss
        
        å¯¹éš¾åˆ†æ ·æœ¬ç»™äºˆæ›´é«˜æƒé‡
        """
        bce = F.binary_cross_entropy(pred, target, reduction='none')
        pt = torch.where(target == 1, pred, 1 - pred)
        focal_weight = (1 - pt) ** gamma
        return (focal_weight * bce).mean()
    
    def forward(
        self,
        signal,  # IntuitionSignal
        targets: Dict[str, torch.Tensor],
        layer_idx: int,
        total_layers: int = 36,
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ€»æŸå¤±
        """
        can_exit = targets["can_exit"]
        is_ood = targets["is_ood"]
        cognitive_load = targets["cognitive_load"]
        difficulty = targets["difficulty"]
        
        # 1. é€€å‡ºå†³ç­–æŸå¤±ï¼ˆæœ€é‡è¦ï¼‰
        # p_confidenté«˜ â†’ åº”è¯¥é€€å‡ºï¼ˆå¦‚æœcan_exit=Trueï¼‰
        exit_pred = signal.p_confident
        exit_loss = self.focal_bce_loss(exit_pred, can_exit, self.focal_gamma)
        
        # 2. ç½®ä¿¡åº¦æ ¡å‡†æŸå¤±
        # ç½®ä¿¡åº¦åº”è¯¥åæ˜ çœŸå®çš„é€€å‡ºå®‰å…¨æ€§
        confidence_target = can_exit * (1.0 - difficulty * 0.3)  # éš¾åº¦è¶Šé«˜ï¼Œç½®ä¿¡åº¦åº”è¯¥è¶Šä½
        confidence_loss = F.mse_loss(signal.p_confident, confidence_target)
        
        # 3. è®¤çŸ¥è´Ÿè·å›å½’æŸå¤±
        # ä½¿ç”¨Huber lossï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
        cognitive_pred = 1.0 - signal.p_confident  # ç®€å•è¿‘ä¼¼ï¼šä½ç½®ä¿¡åº¦=é«˜è®¤çŸ¥è´Ÿè·
        cognitive_loss = F.smooth_l1_loss(cognitive_pred, cognitive_load)
        
        # 4. OODæ£€æµ‹æŸå¤±
        ood_loss = F.binary_cross_entropy(signal.p_ood, is_ood)
        
        # 5. å¹»è§‰æ£€æµ‹æŸå¤±
        # hallucination â‰ˆ ä½ç½®ä¿¡åº¦ + éOOD
        halluc_target = (1.0 - can_exit) * (1.0 - is_ood)
        halluc_loss = F.binary_cross_entropy(signal.p_hallucination, halluc_target)
        
        # åŠ æƒæ€»æŸå¤±
        total = (
            self.exit_weight * exit_loss +
            self.confidence_weight * confidence_loss +
            self.cognitive_load_weight * cognitive_loss +
            0.5 * ood_loss +
            0.5 * halluc_loss
        )
        
        return {
            "total": total,
            "exit": exit_loss,
            "confidence": confidence_loss,
            "cognitive_load": cognitive_loss,
            "ood": ood_loss,
            "hallucination": halluc_loss,
        }


class AdaptiveTrainer:
    """
    è‡ªé€‚åº”è®­ç»ƒå™¨
    
    ç›®æ ‡ï¼šé€€å‡ºç²¾åº¦ 95%+
    """
    
    def __init__(
        self,
        config: AdaptiveTrainingConfig = None,
        intuition_config: IntuitionConfig = None,
    ):
        self.config = config or AdaptiveTrainingConfig()
        
        # è®¾å¤‡
        if self.config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(self.config.device)
        
        logger.info(f"Using device: {self.device}")
        
        # æ¨¡å‹
        intuition_config = intuition_config or IntuitionConfig(dropout=self.config.dropout)
        self.model = IntuitionNetwork(intuition_config).to(self.device)
        
        # æŸå¤±å‡½æ•°
        self.loss_fn = AdaptiveLoss(
            exit_weight=self.config.exit_weight,
            confidence_weight=self.config.confidence_weight,
            cognitive_load_weight=self.config.cognitive_load_weight,
            label_smoothing=self.config.label_smoothing,
        )
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = None  # åœ¨train()ä¸­åˆå§‹åŒ–
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_loss = float('inf')
        self.best_exit_precision = 0.0
        self.patience_counter = 0
        self.history = {"train_loss": [], "val_loss": [], "exit_precision": [], "exit_recall": []}
    
    def load_data(self) -> Tuple[DataLoader, DataLoader]:
        """åŠ è½½æ•°æ®"""
        data_path = Path(self.config.data_path)
        
        if not data_path.exists():
            raise FileNotFoundError(f"Training data not found: {data_path}")
        
        with open(data_path, 'r') as f:
            raw_data = json.load(f)
        
        num_layers = raw_data.get("num_layers", 36)
        samples = raw_data.get("samples", [])
        
        logger.info(f"Loaded {len(samples)} tokens, {num_layers} layers")
        
        # åˆ†å‰²è®­ç»ƒ/éªŒè¯
        val_size = int(len(samples) * self.config.val_split)
        train_samples = samples[val_size:]
        val_samples = samples[:val_size]
        
        train_dataset = AdaptiveDataset(train_samples, num_layers, compute_weights=True)
        val_dataset = AdaptiveDataset(val_samples, num_layers, compute_weights=False)
        
        # ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨
        if train_dataset.weights:
            sampler = WeightedRandomSampler(
                weights=train_dataset.weights,
                num_samples=len(train_dataset),
                replacement=True,
            )
            train_loader = DataLoader(
                train_dataset,
                batch_size=self.config.batch_size,
                sampler=sampler,
                num_workers=0,
            )
        else:
            train_loader = DataLoader(
                train_dataset,
                batch_size=self.config.batch_size,
                shuffle=True,
                num_workers=0,
            )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0,
        )
        
        return train_loader, val_loader
    
    def train_epoch(self, loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        loss_components = {"exit": 0.0, "confidence": 0.0, "cognitive_load": 0.0, "ood": 0.0}
        
        for batch in loader:
            features = batch["features"].to(self.device)
            layer_idx = batch["layer_idx"][0].item()
            
            # å‰å‘ä¼ æ’­
            signal = self.model(features, layer_idx)
            
            # è®¡ç®—æŸå¤±
            targets = {
                "can_exit": batch["can_exit"].to(self.device),
                "is_ood": batch["is_ood"].to(self.device),
                "cognitive_load": batch["cognitive_load"].to(self.device),
                "difficulty": batch["difficulty"].to(self.device),
            }
            
            losses = self.loss_fn(signal, targets, layer_idx)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            losses["total"].backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += losses["total"].item()
            for k in loss_components:
                if k in losses:
                    loss_components[k] += losses[k].item()
        
        n_batches = len(loader)
        return {
            "total": total_loss / n_batches,
            **{k: v / n_batches for k, v in loss_components.items()},
        }
    
    @torch.no_grad()
    def validate(self, loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0.0
        
        # ç²¾ç¡®ç‡å’Œå¬å›ç‡ç»Ÿè®¡
        true_positives = 0   # æ­£ç¡®é¢„æµ‹é€€å‡ºä¸”ç¡®å®å¯ä»¥é€€å‡º
        false_positives = 0  # é¢„æµ‹é€€å‡ºä½†ä¸èƒ½é€€å‡º
        false_negatives = 0  # å¯ä»¥é€€å‡ºä½†æ²¡é¢„æµ‹é€€å‡º
        true_negatives = 0   # ä¸èƒ½é€€å‡ºä¸”æ²¡é¢„æµ‹é€€å‡º
        
        total_samples = 0
        
        for batch in loader:
            features = batch["features"].to(self.device)
            layer_idx = batch["layer_idx"][0].item()
            
            signal = self.model(features, layer_idx)
            
            targets = {
                "can_exit": batch["can_exit"].to(self.device),
                "is_ood": batch["is_ood"].to(self.device),
                "cognitive_load": batch["cognitive_load"].to(self.device),
                "difficulty": batch["difficulty"].to(self.device),
            }
            
            losses = self.loss_fn(signal, targets, layer_idx)
            total_loss += losses["total"].item()
            
            # è®¡ç®—é€€å‡ºå†³ç­–ç»Ÿè®¡
            exit_pred = (signal.p_confident > 0.5).float()
            can_exit = targets["can_exit"]
            
            true_positives += ((exit_pred == 1) & (can_exit == 1)).sum().item()
            false_positives += ((exit_pred == 1) & (can_exit == 0)).sum().item()
            false_negatives += ((exit_pred == 0) & (can_exit == 1)).sum().item()
            true_negatives += ((exit_pred == 0) & (can_exit == 0)).sum().item()
            
            total_samples += features.shape[0]
        
        n_batches = len(loader)
        
        # è®¡ç®—æŒ‡æ ‡
        precision = true_positives / max(true_positives + false_positives, 1)
        recall = true_positives / max(true_positives + false_negatives, 1)
        f1 = 2 * precision * recall / max(precision + recall, 1e-6)
        accuracy = (true_positives + true_negatives) / max(total_samples, 1)
        
        # é€€å‡ºç‡
        exit_rate = (true_positives + false_positives) / max(total_samples, 1)
        
        return {
            "total": total_loss / n_batches,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "accuracy": accuracy,
            "exit_rate": exit_rate,
        }
    
    def save_checkpoint(self, path: str, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        Path(self.config.save_dir).mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "config": self.config,
            "history": self.history,
            "best_exit_precision": self.best_exit_precision,
        }
        
        torch.save(checkpoint, path)
        logger.info(f"Saved checkpoint: {path}")
    
    def train(self) -> Dict[str, List]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        train_loader, val_loader = self.load_data()
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=10,
            T_mult=2,
            eta_min=self.config.min_lr,
        )
        
        logger.info(f"Starting training for {self.config.epochs} epochs")
        logger.info(f"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}")
        logger.info(f"Target: Exit Precision >= 95%")
        
        for epoch in range(self.config.epochs):
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_metrics = self.validate(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.history["train_loss"].append(train_metrics["total"])
            self.history["val_loss"].append(val_metrics["total"])
            self.history["exit_precision"].append(val_metrics["precision"])
            self.history["exit_recall"].append(val_metrics["recall"])
            
            # æ—¥å¿—
            logger.info(
                f"Epoch {epoch+1}/{self.config.epochs} | "
                f"Train Loss: {train_metrics['total']:.4f} | "
                f"Val Loss: {val_metrics['total']:.4f} | "
                f"Precision: {val_metrics['precision']*100:.2f}% | "
                f"Recall: {val_metrics['recall']*100:.2f}% | "
                f"F1: {val_metrics['f1']*100:.2f}% | "
                f"LR: {current_lr:.2e}"
            )
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if val_metrics["precision"] >= 0.95:
                logger.info(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡ç²¾åº¦ {val_metrics['precision']*100:.2f}%!")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºprecisionï¼‰
            if val_metrics["precision"] > self.best_exit_precision:
                self.best_exit_precision = val_metrics["precision"]
                self.patience_counter = 0
                self.save_checkpoint(
                    f"{self.config.save_dir}/intuition_network_best_v9.pt",
                    is_best=True
                )
            else:
                self.patience_counter += 1
            
            # æ—©åœ
            if self.patience_counter >= self.config.patience:
                logger.info(f"Early stopping at epoch {epoch+1}")
                break
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(f"{self.config.save_dir}/intuition_network_final_v9.pt")
        
        logger.info(f"Training complete. Best Precision: {self.best_exit_precision*100:.2f}%")
        
        return self.history


def train_adaptive_model(
    data_path: str = "sedac_v9_augmented_data.json",
    epochs: int = 100,
    batch_size: int = 128,
):
    """ä¾¿æ·è®­ç»ƒå‡½æ•°"""
    config = AdaptiveTrainingConfig(
        data_path=data_path,
        epochs=epochs,
        batch_size=batch_size,
    )
    
    trainer = AdaptiveTrainer(config)
    history = trainer.train()
    
    return history


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    train_adaptive_model()
