"""
SEDAC V9.0 - å·¥ä¸šçº§é›†æˆå™¨ (Industrial Integrator)

åŸºäº NVIDIA/OpenAI ç”Ÿäº§çº§æ ‡å‡†çš„å®Œæ•´é›†æˆæ–¹æ¡ˆ

æ”¯æŒä¸‰ç§ç­–ç•¥ï¼š
- æ–¹æ¡ˆA (Safe): é”šç‚¹å±‚ + KV-Onlyè®¡ç®— - ç¨³å®šæ€§ä¼˜å…ˆ
- æ–¹æ¡ˆB (Fast): Ghost KVé¢„æµ‹ - æ€§èƒ½ä¼˜å…ˆ  
- æ–¹æ¡ˆC (Ultimate): Per-Tokenæ··åˆç­–ç•¥ - æè‡´ä¼˜åŒ–

æ¶æ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEDAC V9.0 Industrial                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input â†’ TokenRouter â†’ LayerDecision â†’ Execution â†’ Output   â”‚
â”‚              â†“              â†“              â†“                â”‚
â”‚         Split Batch    SEDAC Engine   KV Strategy           â”‚
â”‚              â†“              â†“              â†“                â”‚
â”‚         Active/Exit    Confidence    Full/KV-Only/Ghost     â”‚
â”‚                              â†“                              â”‚
â”‚                      AttentionSinks                         â”‚
â”‚                      (Safety Net)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

from __future__ import annotations
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any, List, Callable, Union
from enum import Enum, auto
import logging
import time
from contextlib import contextmanager

logger = logging.getLogger(__name__)

# å¯¼å…¥ SEDAC V9.0 ç»„ä»¶
from sedac.v9.kv_cache_manager import (
    KVCacheManager, KVOnlyProjection, AdaptiveLayerScheduler,
    SkipMode, LayerDecision, create_kv_cache_manager, create_layer_scheduler,
)
from sedac.v9.ghost_kv import (
    GhostKVGenerator, GhostKVConfig, GhostKVManager,
    create_ghost_kv_manager,
)
from sedac.v9.token_router import (
    TokenRouter, TokenState, RaggedBatch, RouterState,
    create_token_router,
)
from sedac.v9.attention_sinks import (
    AttentionSinkProtector, AnchorLayerManager, ProtectionLevel,
    create_attention_sink_protector,
)
from sedac.v9.fused_gpu_kernel import (
    FusedSEDACEngine, create_fused_engine,
)


class IntegrationStrategy(Enum):
    """é›†æˆç­–ç•¥"""
    SAFE = auto()       # æ–¹æ¡ˆA: é”šç‚¹å±‚ + KV-Only (ç¨³å®šæ€§ä¼˜å…ˆ)
    FAST = auto()       # æ–¹æ¡ˆB: Ghost KVé¢„æµ‹ (æ€§èƒ½ä¼˜å…ˆ)
    ULTIMATE = auto()   # æ–¹æ¡ˆC: Per-Tokenæ··åˆ (æè‡´ä¼˜åŒ–)
    ADAPTIVE = auto()   # è‡ªé€‚åº”ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨é€‰æ‹©


@dataclass
class IndustrialConfig:
    """å·¥ä¸šçº§é…ç½®"""
    # æ¨¡å‹å‚æ•°
    hidden_size: int = 4096
    num_layers: int = 32
    num_heads: int = 32
    head_dim: int = 128
    vocab_size: int = 32000
    max_seq_len: int = 4096
    
    # ç­–ç•¥é…ç½®
    strategy: IntegrationStrategy = IntegrationStrategy.SAFE
    anchor_interval: int = 4          # é”šç‚¹å±‚é—´éš”
    num_sink_tokens: int = 4          # Attention Sinkæ•°é‡
    
    # å†³ç­–é˜ˆå€¼
    exit_threshold: float = 0.7       # é€€å‡ºé˜ˆå€¼
    kv_only_threshold: float = 0.5    # KV-Onlyé˜ˆå€¼
    ghost_threshold: float = 0.3      # Ghost KVé˜ˆå€¼
    
    # æ€§èƒ½é…ç½®
    use_cuda_graphs: bool = True      # CUDA GraphåŠ é€Ÿ
    use_triton: bool = True           # Tritonç®—å­
    profile_enabled: bool = False     # æ€§èƒ½åˆ†æ
    
    # å®‰å…¨é…ç½®
    max_skip_ratio: float = 0.6       # æœ€å¤§è·³å±‚æ¯”ä¾‹
    min_compute_layers: int = 8       # æœ€å°‘è®¡ç®—å±‚æ•°
    force_first_n: int = 2            # å¼ºåˆ¶è®¡ç®—å‰Nå±‚
    force_last_n: int = 2             # å¼ºåˆ¶è®¡ç®—åNå±‚


@dataclass 
class LayerOutput:
    """å±‚è¾“å‡º"""
    hidden_states: torch.Tensor
    key: Optional[torch.Tensor] = None
    value: Optional[torch.Tensor] = None
    skip_mode: SkipMode = SkipMode.FULL_COMPUTE
    confidence: float = 0.0
    latency_ms: float = 0.0


@dataclass
class InferenceMetrics:
    """æ¨ç†æŒ‡æ ‡"""
    total_layers: int = 0
    computed_layers: int = 0
    kv_only_layers: int = 0
    ghost_layers: int = 0
    skipped_layers: int = 0
    total_latency_ms: float = 0.0
    layer_latencies: List[float] = field(default_factory=list)
    
    @property
    def skip_ratio(self) -> float:
        if self.total_layers == 0:
            return 0.0
        return (self.kv_only_layers + self.ghost_layers + self.skipped_layers) / self.total_layers
    
    @property
    def theoretical_speedup(self) -> float:
        if self.computed_layers == 0:
            return 1.0
        return self.total_layers / self.computed_layers
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_layers": self.total_layers,
            "computed_layers": self.computed_layers,
            "kv_only_layers": self.kv_only_layers,
            "ghost_layers": self.ghost_layers,
            "skipped_layers": self.skipped_layers,
            "skip_ratio": f"{self.skip_ratio*100:.1f}%",
            "theoretical_speedup": f"{self.theoretical_speedup:.2f}x",
            "total_latency_ms": f"{self.total_latency_ms:.2f}ms",
            "avg_layer_latency_ms": f"{sum(self.layer_latencies)/max(len(self.layer_latencies),1):.3f}ms",
        }


class SEDACLayerWrapper(nn.Module):
    """
    SEDACå±‚åŒ…è£…å™¨
    
    åŒ…è£…åŸå§‹TransformerLayerï¼Œæ³¨å…¥SEDACå†³ç­–é€»è¾‘
    """
    
    def __init__(
        self,
        original_layer: nn.Module,
        layer_idx: int,
        integrator: 'IndustrialIntegrator',
    ):
        super().__init__()
        self.original_layer = original_layer
        self.layer_idx = layer_idx
        self.integrator = integrator
        
        # æå–å±‚ç»„ä»¶ï¼ˆå‡è®¾æ ‡å‡†Transformerç»“æ„ï¼‰
        self._extract_components()
    
    def _extract_components(self):
        """æå–å±‚ç»„ä»¶"""
        # å°è¯•å¸¸è§çš„å±æ€§å
        self.self_attn = getattr(self.original_layer, 'self_attn', None)
        self.mlp = getattr(self.original_layer, 'mlp', None)
        self.ffn = getattr(self.original_layer, 'ffn', self.mlp)
        self.input_layernorm = getattr(self.original_layer, 'input_layernorm', None)
        self.post_attention_layernorm = getattr(self.original_layer, 'post_attention_layernorm', None)
        
        # LLaMAé£æ ¼
        if self.self_attn is None:
            self.self_attn = getattr(self.original_layer, 'attention', None)
        
        # GPTé£æ ¼
        if self.self_attn is None:
            self.self_attn = getattr(self.original_layer, 'attn', None)
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = True,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        å¸¦SEDACå†³ç­–çš„å‰å‘ä¼ æ’­
        """
        start_time = time.perf_counter()
        
        # 1. è·å–SEDACå†³ç­–
        decision = self.integrator.get_layer_decision(
            self.layer_idx, hidden_states
        )
        
        # 2. æ£€æŸ¥é”šç‚¹å±‚
        if self.integrator.is_anchor_layer(self.layer_idx):
            decision.skip_mode = SkipMode.FULL_COMPUTE
            decision.reason = "Anchor layer (forced)"
        
        # 3. æ‰§è¡Œåˆ†æ”¯
        if decision.skip_mode == SkipMode.FULL_COMPUTE:
            # å®Œæ•´è®¡ç®—
            output, present_kv = self._full_compute(
                hidden_states, attention_mask, position_ids, past_key_value, use_cache, **kwargs
            )
            self.integrator.metrics.computed_layers += 1
            
        elif decision.skip_mode == SkipMode.KV_ONLY:
            # åªè®¡ç®—KV
            output, present_kv = self._kv_only_compute(
                hidden_states, attention_mask, position_ids, past_key_value, **kwargs
            )
            self.integrator.metrics.kv_only_layers += 1
            
        elif decision.skip_mode == SkipMode.FFN_SKIP:
            # è·³è¿‡FFN
            output, present_kv = self._ffn_skip_compute(
                hidden_states, attention_mask, position_ids, past_key_value, use_cache, **kwargs
            )
            self.integrator.metrics.kv_only_layers += 1
            
        else:  # FULL_SKIP
            # Ghost KVæˆ–å®Œå…¨è·³è¿‡
            output, present_kv = self._ghost_or_skip(
                hidden_states, past_key_value, **kwargs
            )
            self.integrator.metrics.ghost_layers += 1
        
        self.integrator.metrics.total_layers += 1
        
        # è®°å½•å»¶è¿Ÿ
        latency_ms = (time.perf_counter() - start_time) * 1000
        self.integrator.metrics.layer_latencies.append(latency_ms)
        self.integrator.metrics.total_latency_ms += latency_ms
        
        return output, present_kv
    
    def _full_compute(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        position_ids: Optional[torch.Tensor],
        past_key_value: Optional[Tuple],
        use_cache: bool,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[Tuple]]:
        """å®Œæ•´è®¡ç®—"""
        # è°ƒç”¨åŸå§‹å±‚
        outputs = self.original_layer(
            hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            use_cache=use_cache,
            **kwargs,
        )
        
        if isinstance(outputs, tuple):
            return outputs[0], outputs[1] if len(outputs) > 1 else None
        return outputs, None
    
    def _kv_only_compute(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        position_ids: Optional[torch.Tensor],
        past_key_value: Optional[Tuple],
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[Tuple]]:
        """
        åªè®¡ç®—KVï¼Œè·³è¿‡Attention Scoreå’ŒFFN
        
        è¿™æ˜¯æ–¹æ¡ˆAçš„æ ¸å¿ƒï¼šä¿æŒKV Cacheè¿ç»­æ€§
        """
        # ä½¿ç”¨KV-OnlyæŠ•å½±
        key, value = self.integrator.kv_projections[self.layer_idx](hidden_states)
        
        # æ›´æ–°KV Cache
        if past_key_value is not None:
            key = torch.cat([past_key_value[0], key], dim=2)
            value = torch.cat([past_key_value[1], value], dim=2)
        
        present_kv = (key, value)
        
        # ç›´æ¥è¿”å›æ®‹å·®ï¼ˆè·³è¿‡è¿™ä¸€å±‚çš„è®¡ç®—ï¼‰
        return hidden_states, present_kv
    
    def _ffn_skip_compute(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        position_ids: Optional[torch.Tensor],
        past_key_value: Optional[Tuple],
        use_cache: bool,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[Tuple]]:
        """
        åªè·³è¿‡FFNï¼Œæ‰§è¡ŒSelf-Attention
        """
        residual = hidden_states
        
        # LayerNorm
        if self.input_layernorm is not None:
            hidden_states = self.input_layernorm(hidden_states)
        
        # Self-Attention
        if self.self_attn is not None:
            attn_output = self.self_attn(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                use_cache=use_cache,
                **kwargs,
            )
            
            if isinstance(attn_output, tuple):
                attn_output, present_kv = attn_output[0], attn_output[1] if len(attn_output) > 1 else None
            else:
                present_kv = None
            
            # æ®‹å·®è¿æ¥
            hidden_states = residual + attn_output
        else:
            present_kv = None
        
        # è·³è¿‡FFNï¼Œç›´æ¥è¿”å›
        return hidden_states, present_kv
    
    def _ghost_or_skip(
        self,
        hidden_states: torch.Tensor,
        past_key_value: Optional[Tuple],
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[Tuple]]:
        """
        ä½¿ç”¨Ghost KVæˆ–å®Œå…¨è·³è¿‡
        
        è¿™æ˜¯æ–¹æ¡ˆBçš„æ ¸å¿ƒï¼šTinyMLPé¢„æµ‹KV
        """
        strategy = self.integrator.config.strategy
        
        if strategy in [IntegrationStrategy.FAST, IntegrationStrategy.ULTIMATE]:
            # ä½¿ç”¨Ghost KV
            prev_key = past_key_value[0] if past_key_value else None
            prev_value = past_key_value[1] if past_key_value else None
            
            ghost_key, ghost_value = self.integrator.ghost_manager.generate_ghost_kv(
                hidden_states, self.layer_idx, prev_key, prev_value
            )
            
            # æ‹¼æ¥å†å²KV
            if past_key_value is not None:
                ghost_key = torch.cat([past_key_value[0], ghost_key], dim=2)
                ghost_value = torch.cat([past_key_value[1], ghost_value], dim=2)
            
            present_kv = (ghost_key, ghost_value)
        else:
            # å®Œå…¨è·³è¿‡ï¼ˆå¤ç”¨ä¸Šä¸€å±‚KVï¼‰
            present_kv = past_key_value
        
        # è¿”å›æ®‹å·®
        return hidden_states, present_kv


class IndustrialIntegrator:
    """
    SEDAC V9.0 å·¥ä¸šçº§é›†æˆå™¨
    
    æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œæä¾›ç”Ÿäº§çº§æ¥å£
    """
    
    def __init__(self, config: IndustrialConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        # æ¨ç†æŒ‡æ ‡
        self.metrics = InferenceMetrics()
        
        # çŠ¶æ€
        self._prev_hidden: Optional[torch.Tensor] = None
        self._layer_decisions: Dict[int, LayerDecision] = {}
        
        logger.info(f"IndustrialIntegrator initialized with strategy: {config.strategy.name}")
    
    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰SEDACç»„ä»¶"""
        cfg = self.config
        
        # 1. KV Cache Manager (çŠ¶æ€ç®¡ç†)
        self.kv_manager = create_kv_cache_manager(
            num_layers=cfg.num_layers,
        )
        
        # 1.1 KV-Only Projections (æ¯å±‚ä¸€ä¸ª)
        from sedac.v9.kv_cache_manager import KVOnlyProjection
        self.kv_projections = nn.ModuleList([
            KVOnlyProjection(cfg.hidden_size, cfg.num_heads, cfg.head_dim)
            for _ in range(cfg.num_layers)
        ])
        
        # 2. Layer Scheduler
        self.scheduler = create_layer_scheduler(
            num_layers=cfg.num_layers,
        )
        
        # 3. Ghost KV Manager (æ–¹æ¡ˆB/C)
        if cfg.strategy in [IntegrationStrategy.FAST, IntegrationStrategy.ULTIMATE, IntegrationStrategy.ADAPTIVE]:
            self.ghost_manager = create_ghost_kv_manager(
                hidden_size=cfg.hidden_size,
                num_heads=cfg.num_heads,
                head_dim=cfg.head_dim,
                num_layers=cfg.num_layers,
                strategy="ghost",
            )
        else:
            self.ghost_manager = None
        
        # 4. Token Router (æ–¹æ¡ˆC)
        if cfg.strategy in [IntegrationStrategy.ULTIMATE, IntegrationStrategy.ADAPTIVE]:
            self.token_router = create_token_router(
                hidden_size=cfg.hidden_size,
                num_layers=cfg.num_layers,
                anchor_interval=cfg.anchor_interval,
            )
        else:
            self.token_router = None
        
        # 5. Attention Sink Protector
        self.sink_protector = create_attention_sink_protector(
            num_layers=cfg.num_layers,
            anchor_interval=cfg.anchor_interval,
            num_sink_tokens=cfg.num_sink_tokens,
        )
        
        # 6. Fused GPU Engine
        if cfg.use_triton:
            self.fused_engine = create_fused_engine(
                vocab_size=cfg.vocab_size,
                hidden_size=cfg.hidden_size,
            )
        else:
            self.fused_engine = None
    
    def is_anchor_layer(self, layer_idx: int) -> bool:
        """æ˜¯å¦æ˜¯é”šç‚¹å±‚"""
        cfg = self.config
        
        # å¼ºåˆ¶è®¡ç®—å‰Nå±‚
        if layer_idx < cfg.force_first_n:
            return True
        
        # å¼ºåˆ¶è®¡ç®—åNå±‚
        if layer_idx >= cfg.num_layers - cfg.force_last_n:
            return True
        
        # é”šç‚¹å±‚
        return self.sink_protector.anchor_manager.is_anchor(layer_idx)
    
    def get_layer_decision(
        self,
        layer_idx: int,
        hidden_states: torch.Tensor,
        logits: Optional[torch.Tensor] = None,
    ) -> LayerDecision:
        """
        è·å–å±‚å†³ç­–
        
        åŸºäºSEDACå¼•æ“çš„æ™ºèƒ½å†³ç­–
        """
        cfg = self.config
        
        # è®¡ç®—ç½®ä¿¡åº¦å’Œè®¤çŸ¥è´Ÿè·
        if self.fused_engine is not None and logits is not None:
            # ä½¿ç”¨Fused GPU Engineï¼ˆé›¶CPUåŒæ­¥ï¼‰
            entropy, confidence, exit_mask, cognitive_load = self.fused_engine.fused_decision(
                logits, hidden_states, 
                self._prev_hidden if self._prev_hidden is not None else hidden_states,
                layer_idx, cfg.num_layers, cfg.exit_threshold,
            )
            avg_confidence = confidence.mean().item()
            avg_cognitive = cognitive_load.mean().item()
        else:
            # ç®€åŒ–è®¡ç®—
            avg_confidence = self._estimate_confidence(hidden_states)
            avg_cognitive = 1.0 - avg_confidence
        
        # å±‚è¿›åº¦
        layer_progress = layer_idx / (cfg.num_layers - 1)
        
        # å†³ç­–é€»è¾‘
        decision = self.scheduler.make_decision(layer_idx, avg_confidence, avg_cognitive)
        
        # åº”ç”¨ç­–ç•¥çº¦æŸ
        decision = self._apply_strategy_constraints(decision, layer_idx, avg_confidence)
        
        # ç¼“å­˜
        self._layer_decisions[layer_idx] = decision
        self._prev_hidden = hidden_states.detach()
        
        return decision
    
    def _estimate_confidence(self, hidden_states: torch.Tensor) -> float:
        """ç®€åŒ–çš„ç½®ä¿¡åº¦ä¼°è®¡"""
        # åŸºäºhidden statesçš„æ–¹å·®
        var = hidden_states.var().item()
        # ä½æ–¹å·® = é«˜ç½®ä¿¡åº¦
        confidence = 1.0 / (1.0 + var)
        return min(max(confidence, 0.0), 1.0)
    
    def _apply_strategy_constraints(
        self,
        decision: LayerDecision,
        layer_idx: int,
        confidence: float,
    ) -> LayerDecision:
        """åº”ç”¨ç­–ç•¥çº¦æŸ"""
        cfg = self.config
        
        # æ£€æŸ¥æœ€å¤§è·³å±‚æ¯”ä¾‹
        current_skip_ratio = self.metrics.skip_ratio
        if current_skip_ratio >= cfg.max_skip_ratio:
            decision.skip_mode = SkipMode.FULL_COMPUTE
            decision.reason = "Max skip ratio reached"
            return decision
        
        # æ£€æŸ¥æœ€å°‘è®¡ç®—å±‚æ•°
        if self.metrics.computed_layers < cfg.min_compute_layers:
            remaining_layers = cfg.num_layers - layer_idx
            needed_computes = cfg.min_compute_layers - self.metrics.computed_layers
            if remaining_layers <= needed_computes:
                decision.skip_mode = SkipMode.FULL_COMPUTE
                decision.reason = "Min compute layers constraint"
                return decision
        
        # æ ¹æ®ç­–ç•¥è°ƒæ•´
        if cfg.strategy == IntegrationStrategy.SAFE:
            # ä¿å®ˆï¼šåªå…è®¸KV-Onlyï¼Œä¸å…è®¸å®Œå…¨è·³è¿‡
            if decision.skip_mode == SkipMode.FULL_SKIP:
                decision.skip_mode = SkipMode.KV_ONLY
                decision.reason = "Safe mode: downgrade to KV-Only"
        
        elif cfg.strategy == IntegrationStrategy.FAST:
            # æ¿€è¿›ï¼šä½ç½®ä¿¡åº¦ç›´æ¥Ghost KV
            if confidence < cfg.ghost_threshold and decision.skip_mode != SkipMode.FULL_COMPUTE:
                decision.skip_mode = SkipMode.FULL_SKIP  # ä½¿ç”¨Ghost KV
                decision.reason = "Fast mode: use Ghost KV"
        
        return decision
    
    def wrap_model(self, model: nn.Module) -> nn.Module:
        """
        åŒ…è£…æ•´ä¸ªæ¨¡å‹
        
        è‡ªåŠ¨è¯†åˆ«å¹¶æ›¿æ¢TransformerLayer
        """
        # æŸ¥æ‰¾layerå®¹å™¨
        layers = None
        for name in ['layers', 'h', 'blocks', 'decoder_layers']:
            if hasattr(model, name):
                layers = getattr(model, name)
                break
        
        if layers is None:
            # å°è¯•åœ¨model.modelä¸­æŸ¥æ‰¾
            if hasattr(model, 'model'):
                for name in ['layers', 'h', 'blocks']:
                    if hasattr(model.model, name):
                        layers = getattr(model.model, name)
                        break
        
        if layers is None:
            logger.warning("Could not find transformer layers. Model not wrapped.")
            return model
        
        # åŒ…è£…æ¯ä¸€å±‚
        wrapped_layers = nn.ModuleList([
            SEDACLayerWrapper(layer, idx, self)
            for idx, layer in enumerate(layers)
        ])
        
        # æ›¿æ¢
        if hasattr(model, 'layers'):
            model.layers = wrapped_layers
        elif hasattr(model, 'h'):
            model.h = wrapped_layers
        elif hasattr(model, 'blocks'):
            model.blocks = wrapped_layers
        elif hasattr(model, 'model') and hasattr(model.model, 'layers'):
            model.model.layers = wrapped_layers
        
        logger.info(f"Wrapped {len(wrapped_layers)} transformer layers with SEDAC")
        return model
    
    def reset_metrics(self):
        """é‡ç½®æŒ‡æ ‡"""
        self.metrics = InferenceMetrics()
        self._prev_hidden = None
        self._layer_decisions.clear()
    
    @contextmanager
    def inference_context(self):
        """æ¨ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        self.reset_metrics()
        try:
            yield self
        finally:
            pass  # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ¸…ç†é€»è¾‘
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ¨ç†æ‘˜è¦"""
        return {
            "config": {
                "strategy": self.config.strategy.name,
                "num_layers": self.config.num_layers,
                "anchor_interval": self.config.anchor_interval,
                "exit_threshold": self.config.exit_threshold,
            },
            "metrics": self.metrics.to_dict(),
            "components": {
                "kv_manager": True,
                "ghost_manager": self.ghost_manager is not None,
                "token_router": self.token_router is not None,
                "fused_engine": self.fused_engine is not None,
            },
        }


class PerTokenIntegrator(IndustrialIntegrator):
    """
    æ–¹æ¡ˆCï¼šPer-Tokençº§åˆ«çš„æ··åˆç­–ç•¥é›†æˆå™¨
    
    æ¯ä¸ªTokenç‹¬ç«‹å†³ç­–ï¼Œæ”¯æŒRagged Tensor
    """
    
    def process_batch_per_token(
        self,
        hidden_states: torch.Tensor,  # [batch, seq_len, hidden]
        layer_idx: int,
        router_state: Optional[RouterState] = None,
    ) -> Tuple[torch.Tensor, RouterState]:
        """
        Per-Tokenå¤„ç†
        
        Returns:
            (output_hidden, updated_state)
        """
        if self.token_router is None:
            raise ValueError("Token router not initialized. Use ULTIMATE or ADAPTIVE strategy.")
        
        # 1. Routeråˆ†å‰²
        active_batch, state = self.token_router.split_batch(
            hidden_states, layer_idx, router_state, 
            confidence_threshold=self.config.exit_threshold,
        )
        
        # 2. å¯¹Active Tokenæ‰§è¡Œå®Œæ•´è®¡ç®—
        if active_batch.total_active > 0:
            # æå–active hidden states
            active_hidden = active_batch.hidden_states
            
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„layerè®¡ç®—
            # computed_hidden = layer(active_hidden)
            # ç®€åŒ–ï¼šæ¨¡æ‹Ÿè®¡ç®—
            computed_hidden = active_hidden + torch.randn_like(active_hidden) * 0.01
        else:
            computed_hidden = torch.empty(0, hidden_states.shape[-1], device=hidden_states.device)
        
        # 3. å¯¹Exit Tokenåªè®¡ç®—KVï¼ˆæˆ–Ghost KVï¼‰
        if state.exit_mask.any():
            exit_positions = state.exit_mask.nonzero(as_tuple=False)
            exit_hidden = hidden_states[exit_positions[:, 0], exit_positions[:, 1]]
            
            # KV-Onlyæˆ–Ghost KV
            if self.config.strategy == IntegrationStrategy.ULTIMATE and self.ghost_manager is not None:
                # Ghost KV
                self.ghost_manager.generate_ghost_kv(
                    exit_hidden.unsqueeze(0), layer_idx
                )
            else:
                # KV-Only
                self.kv_manager.compute_kv_only(layer_idx, exit_hidden.unsqueeze(0))
        
        # 4. åˆå¹¶ç»“æœ
        merged = self.token_router.merge_batch(active_batch, computed_hidden, state)
        
        return merged, state


def create_industrial_integrator(
    strategy: str = "safe",
    hidden_size: int = 4096,
    num_layers: int = 32,
    **kwargs,
) -> IndustrialIntegrator:
    """
    åˆ›å»ºå·¥ä¸šçº§é›†æˆå™¨
    
    Args:
        strategy: "safe", "fast", "ultimate", "adaptive"
        hidden_size: éšè—å±‚å¤§å°
        num_layers: å±‚æ•°
        **kwargs: å…¶ä»–é…ç½®
    """
    strategy_map = {
        "safe": IntegrationStrategy.SAFE,
        "fast": IntegrationStrategy.FAST,
        "ultimate": IntegrationStrategy.ULTIMATE,
        "adaptive": IntegrationStrategy.ADAPTIVE,
    }
    
    config = IndustrialConfig(
        strategy=strategy_map.get(strategy.lower(), IntegrationStrategy.SAFE),
        hidden_size=hidden_size,
        num_layers=num_layers,
        **kwargs,
    )
    
    if config.strategy == IntegrationStrategy.ULTIMATE:
        return PerTokenIntegrator(config)
    return IndustrialIntegrator(config)


def demo_industrial_integrator():
    """æ¼”ç¤ºå·¥ä¸šçº§é›†æˆå™¨"""
    print("=" * 70)
    print("SEDAC V9.0 Industrial Integrator Demo")
    print("=" * 70)
    
    # æµ‹è¯•ä¸‰ç§ç­–ç•¥
    strategies = ["safe", "fast", "ultimate"]
    
    for strategy in strategies:
        print(f"\n{'='*30} Strategy: {strategy.upper()} {'='*30}")
        
        # åˆ›å»ºé›†æˆå™¨
        integrator = create_industrial_integrator(
            strategy=strategy,
            hidden_size=512,
            num_layers=12,
            num_heads=8,
            head_dim=64,
            anchor_interval=4,
        )
        
        # æ¨¡æ‹Ÿæ¨ç†
        with integrator.inference_context():
            # æ¨¡æ‹Ÿ12å±‚çš„å†³ç­–
            hidden = torch.randn(2, 64, 512)
            
            for layer_idx in range(12):
                decision = integrator.get_layer_decision(layer_idx, hidden)
                
                # æ¨¡æ‹ŸæŒ‡æ ‡æ›´æ–°
                if decision.skip_mode == SkipMode.FULL_COMPUTE:
                    integrator.metrics.computed_layers += 1
                elif decision.skip_mode == SkipMode.KV_ONLY:
                    integrator.metrics.kv_only_layers += 1
                else:
                    integrator.metrics.ghost_layers += 1
                integrator.metrics.total_layers += 1
                
                is_anchor = "ğŸ”’" if integrator.is_anchor_layer(layer_idx) else "  "
                print(f"  Layer {layer_idx:2d} {is_anchor}: {decision.skip_mode.name:12s} "
                      f"(conf={decision.confidence:.2f})")
                
                # æ¨¡æ‹Ÿhiddenæ›´æ–°
                hidden = hidden + torch.randn_like(hidden) * 0.05
            
            # è¾“å‡ºæ‘˜è¦
            summary = integrator.get_summary()
            print(f"\n  æ‘˜è¦:")
            for key, value in summary["metrics"].items():
                print(f"    {key}: {value}")
    
    print("\n" + "=" * 70)
    print("Industrial Integrator: ç”Ÿäº§çº§SEDACé›†æˆæ–¹æ¡ˆ")
    print("=" * 70)


if __name__ == "__main__":
    demo_industrial_integrator()
