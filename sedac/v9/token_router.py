"""
SEDAC V9.0 - Token Router (å·¥ä¸šçº§åŠ¨æ€åˆ†ç»„)

è§£å†³"Batchå¹³å‡ä¸»ä¹‰é™·é˜±"ï¼š
- é—®é¢˜ï¼šå¯¹Batchå–å¹³å‡å†³å®šæ˜¯å¦è·³å±‚ï¼Œéš¾æ ·æœ¬æ‹–æ­»ç®€å•æ ·æœ¬
- æ–¹æ¡ˆï¼šToken-level Routerï¼Œæ¯ä¸ªTokenç‹¬ç«‹å†³ç­–ï¼ŒåŠ¨æ€åˆ†ç»„

æ ¸å¿ƒæœºåˆ¶ï¼š
1. Split: æ¯å±‚å‰å°†Batchåˆ†ä¸º Group_Exit å’Œ Group_Continue
2. Execute: åªå¯¹ Group_Continue æ‰§è¡ŒGPUè®¡ç®—
3. Merge: å±‚åå°†ä¸¤ç»„æ•°æ®æ‹¼å›

æ”¯æŒï¼š
- Ragged Tensorï¼ˆå‚å·®å¼ é‡ï¼‰
- Continuous Batchingï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰
- Per-Tokenå†³ç­–ï¼Œæ— "é™ªè·‘"
"""

from __future__ import annotations
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Tuple, NamedTuple
from enum import Enum, auto
import logging

logger = logging.getLogger(__name__)


class TokenState(Enum):
    """TokençŠ¶æ€"""
    ACTIVE = auto()      # æ´»è·ƒï¼Œç»§ç»­è®¡ç®—
    EXITED = auto()      # å·²é€€å‡ºï¼Œç­‰å¾…åˆå¹¶
    ANCHORED = auto()    # é”šç‚¹Tokenï¼Œå¼ºåˆ¶è®¡ç®—


@dataclass
class TokenMetadata:
    """Tokenå…ƒæ•°æ®"""
    token_idx: int
    batch_idx: int
    state: TokenState
    exit_layer: int = -1
    confidence: float = 0.0
    cognitive_load: float = 0.0


class RaggedBatch(NamedTuple):
    """
    å‚å·®å¼ é‡æ‰¹æ¬¡
    
    æ”¯æŒåŒä¸€Batchå†…ä¸åŒTokenå¤„äºä¸åŒè®¡ç®—æ·±åº¦
    """
    hidden_states: torch.Tensor      # [total_active, hidden_size]
    indices: torch.Tensor            # [total_active] - åŸå§‹ä½ç½®ç´¢å¼•
    batch_ids: torch.Tensor          # [total_active] - å±äºå“ªä¸ªbatch
    seq_positions: torch.Tensor      # [total_active] - åºåˆ—å†…ä½ç½®
    
    @property
    def total_active(self) -> int:
        return self.hidden_states.shape[0]


@dataclass
class RouterState:
    """RouterçŠ¶æ€"""
    original_shape: Tuple[int, int, int]  # [batch, seq_len, hidden]
    active_mask: torch.Tensor             # [batch, seq_len] bool
    exit_mask: torch.Tensor               # [batch, seq_len] bool
    exit_hidden: torch.Tensor             # å·²é€€å‡ºTokençš„hidden states
    exit_layers: torch.Tensor             # [batch, seq_len] é€€å‡ºå±‚å·
    confidences: torch.Tensor             # [batch, seq_len] ç½®ä¿¡åº¦


class TokenRouter(nn.Module):
    """
    Tokençº§åˆ«è·¯ç”±å™¨
    
    å®ç°çœŸæ­£çš„Per-TokenåŠ¨æ€è®¡ç®—ï¼Œè§£å†³Batchå¹³å‡ä¸»ä¹‰é—®é¢˜
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_layers: int,
        anchor_interval: int = 4,  # æ¯4å±‚ä¸€ä¸ªé”šç‚¹
        min_active_ratio: float = 0.1,  # æœ€å°‘ä¿æŒ10%æ´»è·ƒ
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.anchor_interval = anchor_interval
        self.min_active_ratio = min_active_ratio
        
        # è½»é‡çº§Routerç½‘ç»œï¼ˆæ¯ä¸ªTokenç‹¬ç«‹å†³ç­–ï¼‰
        self.router_net = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.SiLU(),
            nn.Linear(hidden_size // 4, 2),  # [continue_logit, exit_logit]
        )
        
        # å±‚çº§åç½®ï¼ˆæ·±å±‚æ›´å€¾å‘é€€å‡ºï¼‰
        self.layer_bias = nn.Parameter(torch.zeros(num_layers))
        
        # ç»Ÿè®¡
        self.total_tokens = 0
        self.exited_tokens = 0
        self.layer_exit_counts = [0] * num_layers
    
    def _is_anchor_layer(self, layer_idx: int) -> bool:
        """æ˜¯å¦æ˜¯é”šç‚¹å±‚ï¼ˆå¼ºåˆ¶è®¡ç®—ï¼‰"""
        return layer_idx % self.anchor_interval == 0
    
    def compute_exit_scores(
        self,
        hidden_states: torch.Tensor,  # [batch, seq_len, hidden] æˆ– [N, hidden]
        layer_idx: int,
    ) -> torch.Tensor:
        """
        è®¡ç®—æ¯ä¸ªTokençš„é€€å‡ºåˆ†æ•°
        
        Returns:
            exit_probs: [batch, seq_len] æˆ– [N] - é€€å‡ºæ¦‚ç‡
        """
        original_shape = hidden_states.shape[:-1]
        flat_hidden = hidden_states.view(-1, self.hidden_size)
        
        # Routerå‰å‘
        logits = self.router_net(flat_hidden)  # [N, 2]
        
        # åŠ å…¥å±‚çº§åç½®ï¼ˆæ·±å±‚æ›´å€¾å‘é€€å‡ºï¼‰
        logits[:, 1] += self.layer_bias[layer_idx]
        
        # Softmaxå¾—åˆ°æ¦‚ç‡
        probs = F.softmax(logits, dim=-1)
        exit_probs = probs[:, 1]  # é€€å‡ºæ¦‚ç‡
        
        return exit_probs.view(*original_shape)
    
    def split_batch(
        self,
        hidden_states: torch.Tensor,  # [batch, seq_len, hidden]
        layer_idx: int,
        state: Optional[RouterState] = None,
        confidence_threshold: float = 0.7,
    ) -> Tuple[RaggedBatch, RouterState]:
        """
        å°†Batchåˆ†å‰²ä¸ºç»§ç»­è®¡ç®—ç»„å’Œé€€å‡ºç»„
        
        Args:
            hidden_states: è¾“å…¥hidden states
            layer_idx: å½“å‰å±‚
            state: ä¸Šä¸€å±‚çš„RouterçŠ¶æ€ï¼ˆé¦–å±‚ä¸ºNoneï¼‰
            confidence_threshold: é€€å‡ºé˜ˆå€¼
            
        Returns:
            (active_batch, updated_state)
        """
        batch_size, seq_len, hidden_size = hidden_states.shape
        device = hidden_states.device
        
        # é¦–å±‚åˆå§‹åŒ–çŠ¶æ€
        if state is None:
            state = RouterState(
                original_shape=(batch_size, seq_len, hidden_size),
                active_mask=torch.ones(batch_size, seq_len, dtype=torch.bool, device=device),
                exit_mask=torch.zeros(batch_size, seq_len, dtype=torch.bool, device=device),
                exit_hidden=torch.zeros(batch_size, seq_len, hidden_size, device=device),
                exit_layers=torch.full((batch_size, seq_len), -1, dtype=torch.long, device=device),
                confidences=torch.zeros(batch_size, seq_len, device=device),
            )
        
        # é”šç‚¹å±‚ï¼šå…¨éƒ¨ç»§ç»­
        if self._is_anchor_layer(layer_idx):
            # åªå¤„ç†å½“å‰æ´»è·ƒçš„Token
            active_indices = state.active_mask.nonzero(as_tuple=False)
            
            if active_indices.shape[0] == 0:
                # æ‰€æœ‰Tokenéƒ½å·²é€€å‡º
                return RaggedBatch(
                    hidden_states=torch.empty(0, hidden_size, device=device),
                    indices=torch.empty(0, dtype=torch.long, device=device),
                    batch_ids=torch.empty(0, dtype=torch.long, device=device),
                    seq_positions=torch.empty(0, dtype=torch.long, device=device),
                ), state
            
            batch_ids = active_indices[:, 0]
            seq_positions = active_indices[:, 1]
            active_hidden = hidden_states[batch_ids, seq_positions]
            indices = batch_ids * seq_len + seq_positions
            
            return RaggedBatch(
                hidden_states=active_hidden,
                indices=indices,
                batch_ids=batch_ids,
                seq_positions=seq_positions,
            ), state
        
        # éé”šç‚¹å±‚ï¼šè®¡ç®—é€€å‡ºåˆ†æ•°
        exit_probs = self.compute_exit_scores(hidden_states, layer_idx)
        
        # å†³å®šå“ªäº›Tokené€€å‡º
        should_exit = (exit_probs > confidence_threshold) & state.active_mask
        
        # ä¿è¯æœ€å°æ´»è·ƒæ¯”ä¾‹
        total_active = state.active_mask.sum().item()
        num_exiting = should_exit.sum().item()
        max_exits = int(total_active * (1 - self.min_active_ratio))
        
        if num_exiting > max_exits:
            # é™åˆ¶é€€å‡ºæ•°é‡ï¼šåªè®©ç½®ä¿¡åº¦æœ€é«˜çš„é€€å‡º
            exit_probs_masked = exit_probs.clone()
            exit_probs_masked[~state.active_mask] = -1
            
            flat_probs = exit_probs_masked.view(-1)
            _, top_indices = flat_probs.topk(max_exits)
            
            should_exit = torch.zeros_like(should_exit)
            should_exit.view(-1)[top_indices] = True
            should_exit = should_exit & state.active_mask
        
        # æ›´æ–°çŠ¶æ€
        new_exits = should_exit & ~state.exit_mask
        
        if new_exits.any():
            # è®°å½•é€€å‡ºçš„hidden states
            state.exit_hidden[new_exits] = hidden_states[new_exits]
            state.exit_layers[new_exits] = layer_idx
            state.confidences[new_exits] = exit_probs[new_exits]
            
            # æ›´æ–°mask
            state.exit_mask = state.exit_mask | new_exits
            state.active_mask = state.active_mask & ~new_exits
            
            # ç»Ÿè®¡
            self.exited_tokens += new_exits.sum().item()
            self.layer_exit_counts[layer_idx] += new_exits.sum().item()
        
        self.total_tokens += state.active_mask.numel()
        
        # æå–æ´»è·ƒToken
        active_indices = state.active_mask.nonzero(as_tuple=False)
        
        if active_indices.shape[0] == 0:
            return RaggedBatch(
                hidden_states=torch.empty(0, hidden_size, device=device),
                indices=torch.empty(0, dtype=torch.long, device=device),
                batch_ids=torch.empty(0, dtype=torch.long, device=device),
                seq_positions=torch.empty(0, dtype=torch.long, device=device),
            ), state
        
        batch_ids = active_indices[:, 0]
        seq_positions = active_indices[:, 1]
        active_hidden = hidden_states[batch_ids, seq_positions]
        indices = batch_ids * seq_len + seq_positions
        
        return RaggedBatch(
            hidden_states=active_hidden,
            indices=indices,
            batch_ids=batch_ids,
            seq_positions=seq_positions,
        ), state
    
    def merge_batch(
        self,
        active_batch: RaggedBatch,
        computed_hidden: torch.Tensor,  # [N_active, hidden]
        state: RouterState,
    ) -> torch.Tensor:
        """
        åˆå¹¶è®¡ç®—ç»“æœå’Œå·²é€€å‡ºToken
        
        Returns:
            merged_hidden: [batch, seq_len, hidden]
        """
        batch_size, seq_len, hidden_size = state.original_shape
        device = computed_hidden.device if computed_hidden.numel() > 0 else state.exit_hidden.device
        
        # åˆå§‹åŒ–è¾“å‡º
        merged = state.exit_hidden.clone()
        
        # å¡«å…¥æ´»è·ƒTokençš„è®¡ç®—ç»“æœ
        if active_batch.total_active > 0:
            merged[active_batch.batch_ids, active_batch.seq_positions] = computed_hidden
        
        return merged
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è·¯ç”±ç»Ÿè®¡"""
        total = max(self.total_tokens, 1)
        return {
            "total_tokens": self.total_tokens,
            "exited_tokens": self.exited_tokens,
            "exit_ratio": self.exited_tokens / total,
            "layer_exit_distribution": {
                i: count / max(sum(self.layer_exit_counts), 1)
                for i, count in enumerate(self.layer_exit_counts)
                if count > 0
            },
            "theoretical_speedup": total / max(total - self.exited_tokens, 1),
        }
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.total_tokens = 0
        self.exited_tokens = 0
        self.layer_exit_counts = [0] * self.num_layers


class BatchScheduler:
    """
    Batchè°ƒåº¦å™¨
    
    å®ç°Continuous Batchingï¼Œæ”¯æŒåŠ¨æ€æ·»åŠ /ç§»é™¤è¯·æ±‚
    """
    
    def __init__(
        self,
        max_batch_size: int = 64,
        max_seq_len: int = 4096,
    ):
        self.max_batch_size = max_batch_size
        self.max_seq_len = max_seq_len
        
        # è¯·æ±‚é˜Ÿåˆ—
        self.pending_requests: List[Dict] = []
        self.active_requests: Dict[int, Dict] = {}
        self.completed_requests: List[Dict] = []
        
        self.request_counter = 0
    
    def add_request(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 256,
        priority: int = 0,
    ) -> int:
        """æ·»åŠ æ–°è¯·æ±‚"""
        request_id = self.request_counter
        self.request_counter += 1
        
        self.pending_requests.append({
            "id": request_id,
            "input_ids": input_ids,
            "max_new_tokens": max_new_tokens,
            "priority": priority,
            "generated_tokens": 0,
            "status": "pending",
        })
        
        return request_id
    
    def schedule_batch(self) -> Tuple[List[int], torch.Tensor]:
        """
        è°ƒåº¦ä¸‹ä¸€ä¸ªbatch
        
        Returns:
            (request_ids, batched_input_ids)
        """
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.pending_requests.sort(key=lambda x: -x["priority"])
        
        # é€‰æ‹©è¯·æ±‚
        selected = []
        total_tokens = 0
        
        for req in self.pending_requests[:]:
            seq_len = req["input_ids"].shape[-1]
            if len(selected) < self.max_batch_size and total_tokens + seq_len <= self.max_seq_len * self.max_batch_size:
                selected.append(req)
                self.pending_requests.remove(req)
                total_tokens += seq_len
        
        if not selected:
            return [], None
        
        # Padåˆ°ç›¸åŒé•¿åº¦
        max_len = max(req["input_ids"].shape[-1] for req in selected)
        batched = []
        request_ids = []
        
        for req in selected:
            req_id = req["id"]
            request_ids.append(req_id)
            self.active_requests[req_id] = req
            
            # Pad
            seq_len = req["input_ids"].shape[-1]
            if seq_len < max_len:
                padding = torch.zeros(max_len - seq_len, dtype=req["input_ids"].dtype, device=req["input_ids"].device)
                padded = torch.cat([req["input_ids"].squeeze(0), padding])
            else:
                padded = req["input_ids"].squeeze(0)
            batched.append(padded)
        
        return request_ids, torch.stack(batched)
    
    def complete_request(self, request_id: int, output: torch.Tensor):
        """å®Œæˆè¯·æ±‚"""
        if request_id in self.active_requests:
            req = self.active_requests.pop(request_id)
            req["output"] = output
            req["status"] = "completed"
            self.completed_requests.append(req)
    
    def get_completed(self) -> List[Dict]:
        """è·å–å·²å®Œæˆçš„è¯·æ±‚"""
        completed = self.completed_requests[:]
        self.completed_requests.clear()
        return completed


def create_token_router(
    hidden_size: int = 4096,
    num_layers: int = 32,
    anchor_interval: int = 4,
) -> TokenRouter:
    """åˆ›å»ºToken Router"""
    return TokenRouter(
        hidden_size=hidden_size,
        num_layers=num_layers,
        anchor_interval=anchor_interval,
    )


def demo_token_router():
    """æ¼”ç¤ºToken Router"""
    print("=" * 60)
    print("Token Router Demo: Per-Token Dynamic Computation")
    print("=" * 60)
    
    # é…ç½®
    batch_size = 4
    seq_len = 16
    hidden_size = 256
    num_layers = 12
    
    # åˆ›å»ºRouter
    router = create_token_router(
        hidden_size=hidden_size,
        num_layers=num_layers,
        anchor_interval=4,
    )
    
    # æ¨¡æ‹Ÿè¾“å…¥ï¼ˆä¸åŒéš¾åº¦ï¼‰
    hidden = torch.randn(batch_size, seq_len, hidden_size)
    
    # è®©ç¬¬ä¸€ä¸ªbatchçš„Tokenæ›´"ç¡®å®š"ï¼ˆä½ç†µï¼Œåº”è¯¥æ—©é€€ï¼‰
    hidden[0] = hidden[0] * 0.1  # ä½æ–¹å·®
    # è®©æœ€åä¸€ä¸ªbatchçš„Tokenæ›´"å›°éš¾"ï¼ˆé«˜ç†µï¼Œåº”è¯¥ç»§ç»­ï¼‰
    hidden[-1] = hidden[-1] * 2.0  # é«˜æ–¹å·®
    
    print(f"\nè¾“å…¥: batch_size={batch_size}, seq_len={seq_len}")
    print(f"Layer 0 (Anchor): å¼ºåˆ¶å…¨éƒ¨è®¡ç®—")
    
    state = None
    
    for layer_idx in range(num_layers):
        active_batch, state = router.split_batch(hidden, layer_idx, state, confidence_threshold=0.6)
        
        # æ¨¡æ‹Ÿå±‚è®¡ç®—
        if active_batch.total_active > 0:
            computed = active_batch.hidden_states + torch.randn_like(active_batch.hidden_states) * 0.01
            hidden = router.merge_batch(active_batch, computed, state)
        
        active_ratio = state.active_mask.sum().item() / state.active_mask.numel()
        is_anchor = "ğŸ”’" if router._is_anchor_layer(layer_idx) else "  "
        
        print(f"  Layer {layer_idx:2d} {is_anchor}: active={active_batch.total_active:3d}/{batch_size*seq_len}, "
              f"ratio={active_ratio*100:.1f}%")
    
    # ç»Ÿè®¡
    stats = router.get_statistics()
    print(f"\nç»Ÿè®¡:")
    print(f"  é€€å‡ºæ¯”ä¾‹: {stats['exit_ratio']*100:.1f}%")
    print(f"  ç†è®ºåŠ é€Ÿ: {stats['theoretical_speedup']:.2f}x")
    print(f"\né€€å‡ºå±‚åˆ†å¸ƒ:")
    for layer, ratio in stats['layer_exit_distribution'].items():
        bar = "â–ˆ" * int(ratio * 30)
        print(f"    Layer {layer:2d}: {bar} {ratio*100:.1f}%")
    
    print("\n" + "=" * 60)
    print("Token Router: æ¯ä¸ªTokenç‹¬ç«‹å†³ç­–ï¼Œæ— 'é™ªè·‘'é—®é¢˜")
    print("=" * 60)


if __name__ == "__main__":
    demo_token_router()
