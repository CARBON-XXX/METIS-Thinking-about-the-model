"""
SEDAC V9.0 - Attention Sinks ä¿æŠ¤æœºåˆ¶

è§£å†³"æ³¨æ„åŠ›æ±‡èšç‚¹"é—®é¢˜ï¼š
- é—®é¢˜ï¼šTransformeræåº¦ä¾èµ–é¦–ä¸ªTokenå’Œç³»ç»Ÿæç¤ºï¼Œé”™è¯¯è·³å±‚ä¼šå¯¼è‡´å´©æºƒ
- æ–¹æ¡ˆï¼šå®šä¹‰é”šç‚¹å±‚+åŠ¨æ€æ©ç ï¼Œå¼ºåˆ¶ä¿æŠ¤å…³é”®ä½ç½®

åŸºäºStreamingLLMç ”ç©¶ï¼š
- é¦–ä¸ªTokenæ˜¯"Attention Sink"ï¼Œå¿…é¡»å®Œæ•´è®¡ç®—
- ç³»ç»Ÿæç¤ºï¼ˆSystem Promptï¼‰åŒæ ·å…³é”®
- é”šç‚¹å±‚å¼ºåˆ¶å…¨é‡è®¡ç®—KV

å®ç°ï¼š
1. é”šç‚¹å±‚å®šä¹‰ï¼ˆæ¯Kå±‚å¼ºåˆ¶è®¡ç®—ï¼‰
2. Attention Sink Tokenä¿æŠ¤
3. åŠ¨æ€Attention Maskä¿®æ”¹
4. KVæ±¡æŸ“æ£€æµ‹ä¸ä¿®å¤
"""

from __future__ import annotations
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any, List, Set
from enum import Enum, auto
import logging

logger = logging.getLogger(__name__)


class ProtectionLevel(Enum):
    """ä¿æŠ¤çº§åˆ«"""
    NONE = auto()        # ä¸ä¿æŠ¤ï¼Œå¯è·³è¿‡
    ANCHOR = auto()      # é”šç‚¹å±‚ï¼Œå¼ºåˆ¶è®¡ç®—
    SINK = auto()        # Attention Sinkï¼Œç»å¯¹ä¸å¯è·³
    CRITICAL = auto()    # å…³é”®Tokenï¼ˆå¦‚ç³»ç»Ÿæç¤ºç»“æŸç¬¦ï¼‰


@dataclass
class TokenProtection:
    """Tokenä¿æŠ¤çŠ¶æ€"""
    position: int
    level: ProtectionLevel
    reason: str
    layers_computed: Set[int] = field(default_factory=set)
    kv_valid: bool = True


@dataclass
class LayerProtection:
    """å±‚ä¿æŠ¤çŠ¶æ€"""
    layer_idx: int
    is_anchor: bool
    force_compute_positions: Set[int] = field(default_factory=set)


class AttentionSinkDetector:
    """
    Attention Sinkæ£€æµ‹å™¨
    
    è¯†åˆ«å“ªäº›Tokenæ˜¯"æ±‡èšç‚¹"ï¼Œéœ€è¦ç‰¹æ®Šä¿æŠ¤
    """
    
    # ç³»ç»Ÿæç¤ºç»“æŸæ ‡è®°ï¼ˆå¸¸è§æ ¼å¼ï¼‰
    SYSTEM_END_MARKERS = [
        "</s>", "[/INST]", "<|im_end|>", "<|eot_id|>",
        "###", "Human:", "User:", "Assistant:",
    ]
    
    def __init__(
        self,
        num_sink_tokens: int = 4,     # å‰Nä¸ªTokenä½œä¸ºSink
        protect_system_prompt: bool = True,
        protect_newlines: bool = False,  # æ˜¯å¦ä¿æŠ¤æ¢è¡Œç¬¦
    ):
        self.num_sink_tokens = num_sink_tokens
        self.protect_system_prompt = protect_system_prompt
        self.protect_newlines = protect_newlines
        
        # æ£€æµ‹åˆ°çš„Sinkä½ç½®
        self.sink_positions: Set[int] = set()
        self.system_prompt_end: int = -1
    
    def detect_sinks(
        self,
        input_ids: torch.Tensor,  # [batch, seq_len] æˆ– [seq_len]
        tokenizer: Any = None,
    ) -> List[TokenProtection]:
        """
        æ£€æµ‹Attention Sinks
        
        Returns:
            ä¿æŠ¤åˆ—è¡¨
        """
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)
        
        batch_size, seq_len = input_ids.shape
        protections = []
        
        # 1. é¦–Nä¸ªTokenä½œä¸ºSink
        for i in range(min(self.num_sink_tokens, seq_len)):
            protections.append(TokenProtection(
                position=i,
                level=ProtectionLevel.SINK,
                reason=f"Attention Sink (position {i})",
            ))
            self.sink_positions.add(i)
        
        # 2. æ£€æµ‹ç³»ç»Ÿæç¤ºç»“æŸä½ç½®
        if self.protect_system_prompt and tokenizer is not None:
            for marker in self.SYSTEM_END_MARKERS:
                try:
                    marker_ids = tokenizer.encode(marker, add_special_tokens=False)
                    # åœ¨åºåˆ—ä¸­æœç´¢marker
                    for batch_idx in range(batch_size):
                        seq = input_ids[batch_idx].tolist()
                        for i in range(len(seq) - len(marker_ids) + 1):
                            if seq[i:i+len(marker_ids)] == marker_ids:
                                # æ‰¾åˆ°ç³»ç»Ÿæç¤ºç»“æŸ
                                end_pos = i + len(marker_ids) - 1
                                if end_pos not in self.sink_positions:
                                    protections.append(TokenProtection(
                                        position=end_pos,
                                        level=ProtectionLevel.CRITICAL,
                                        reason=f"System prompt end ({marker})",
                                    ))
                                    self.system_prompt_end = end_pos
                                break
                except:
                    pass
        
        return protections
    
    def is_sink(self, position: int) -> bool:
        """æ£€æŸ¥ä½ç½®æ˜¯å¦æ˜¯Sink"""
        return position in self.sink_positions or position < self.num_sink_tokens
    
    def get_protected_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """
        è·å–ä¿æŠ¤mask
        
        Returns:
            [seq_len] bool tensor, True = å—ä¿æŠ¤
        """
        mask = torch.zeros(seq_len, dtype=torch.bool, device=device)
        
        # Sink Token
        mask[:self.num_sink_tokens] = True
        
        # é¢å¤–ä¿æŠ¤ä½ç½®
        for pos in self.sink_positions:
            if pos < seq_len:
                mask[pos] = True
        
        # ç³»ç»Ÿæç¤ºç»“æŸ
        if self.system_prompt_end >= 0 and self.system_prompt_end < seq_len:
            mask[self.system_prompt_end] = True
        
        return mask


class AnchorLayerManager:
    """
    é”šç‚¹å±‚ç®¡ç†å™¨
    
    å®šä¹‰å“ªäº›å±‚æ˜¯"é”šç‚¹"ï¼Œå¼ºåˆ¶å…¨é‡è®¡ç®—
    """
    
    def __init__(
        self,
        num_layers: int,
        anchor_interval: int = 4,  # æ¯4å±‚ä¸€ä¸ªé”šç‚¹
        first_n_anchors: int = 2,  # å‰Nå±‚å¼ºåˆ¶é”šç‚¹
        last_n_anchors: int = 2,   # åNå±‚å¼ºåˆ¶é”šç‚¹
    ):
        self.num_layers = num_layers
        self.anchor_interval = anchor_interval
        self.first_n_anchors = first_n_anchors
        self.last_n_anchors = last_n_anchors
        
        # è®¡ç®—é”šç‚¹å±‚
        self.anchor_layers: Set[int] = set()
        self._compute_anchors()
    
    def _compute_anchors(self):
        """è®¡ç®—é”šç‚¹å±‚"""
        # å‰Nå±‚
        for i in range(self.first_n_anchors):
            self.anchor_layers.add(i)
        
        # åNå±‚
        for i in range(self.num_layers - self.last_n_anchors, self.num_layers):
            self.anchor_layers.add(i)
        
        # ä¸­é—´æŒ‰é—´éš”
        for i in range(0, self.num_layers, self.anchor_interval):
            self.anchor_layers.add(i)
    
    def is_anchor(self, layer_idx: int) -> bool:
        """æ˜¯å¦æ˜¯é”šç‚¹å±‚"""
        return layer_idx in self.anchor_layers
    
    def get_anchor_mask(self, device: torch.device) -> torch.Tensor:
        """
        è·å–é”šç‚¹mask
        
        Returns:
            [num_layers] bool tensor
        """
        mask = torch.zeros(self.num_layers, dtype=torch.bool, device=device)
        for idx in self.anchor_layers:
            mask[idx] = True
        return mask
    
    def get_skip_candidates(self) -> List[int]:
        """è·å–å¯è·³è¿‡çš„å±‚"""
        return [i for i in range(self.num_layers) if i not in self.anchor_layers]


class DynamicAttentionMask:
    """
    åŠ¨æ€Attention Mask
    
    å½“æ£€æµ‹åˆ°KVæ±¡æŸ“æ—¶ï¼Œä¿®æ”¹Attention Maskè®©æ¨¡å‹å¿½ç•¥è„æ•°æ®
    """
    
    def __init__(
        self,
        num_layers: int,
        max_seq_len: int = 4096,
    ):
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        
        # KVæœ‰æ•ˆæ€§è¿½è¸ªï¼š[layer, seq_pos] -> æ˜¯å¦æœ‰æ•ˆ
        self.kv_validity: Dict[Tuple[int, int], bool] = {}
        
        # æ±¡æŸ“è®¡æ•°
        self.pollution_count = 0
    
    def mark_valid(self, layer_idx: int, positions: torch.Tensor):
        """æ ‡è®°KVä¸ºæœ‰æ•ˆ"""
        for pos in positions.tolist():
            self.kv_validity[(layer_idx, pos)] = True
    
    def mark_invalid(self, layer_idx: int, positions: torch.Tensor):
        """æ ‡è®°KVä¸ºæ— æ•ˆï¼ˆè¢«è·³è¿‡ï¼‰"""
        for pos in positions.tolist():
            self.kv_validity[(layer_idx, pos)] = False
            self.pollution_count += 1
    
    def is_valid(self, layer_idx: int, position: int) -> bool:
        """æ£€æŸ¥KVæ˜¯å¦æœ‰æ•ˆ"""
        return self.kv_validity.get((layer_idx, position), True)
    
    def get_attention_mask(
        self,
        layer_idx: int,
        seq_len: int,
        device: torch.device,
        causal: bool = True,
    ) -> torch.Tensor:
        """
        è·å–åŠ¨æ€Attention Mask
        
        Returns:
            [seq_len, seq_len] mask tensor
            True = å¯ä»¥attend, False = è¢«maskæ‰
        """
        # åŸºç¡€causal mask
        if causal:
            mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device))
        else:
            mask = torch.ones(seq_len, seq_len, dtype=torch.bool, device=device)
        
        # æ ‡è®°æ— æ•ˆä½ç½®
        for pos in range(seq_len):
            if not self.is_valid(layer_idx, pos):
                # è¿™ä¸ªä½ç½®çš„KVè¢«è·³è¿‡äº†ï¼Œmaskæ‰å¯¹å®ƒçš„attention
                mask[:, pos] = False
        
        return mask
    
    def find_nearest_valid(
        self,
        layer_idx: int,
        position: int,
        anchor_manager: AnchorLayerManager,
    ) -> int:
        """
        æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆé”šç‚¹å±‚
        
        å½“æŸå±‚KVæ— æ•ˆæ—¶ï¼Œé‡å®šå‘åˆ°æœ€è¿‘çš„é”šç‚¹å±‚
        """
        # å‘å‰æœç´¢
        for l in range(layer_idx, -1, -1):
            if anchor_manager.is_anchor(l) and self.is_valid(l, position):
                return l
        
        # å‘åæœç´¢ï¼ˆä¸å¤ªå¯èƒ½ï¼Œä½†ä½œä¸ºfallbackï¼‰
        for l in range(layer_idx, self.num_layers):
            if anchor_manager.is_anchor(l) and self.is_valid(l, position):
                return l
        
        return layer_idx  # æ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸå±‚
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡"""
        total_entries = len(self.kv_validity)
        valid_count = sum(1 for v in self.kv_validity.values() if v)
        
        return {
            "total_entries": total_entries,
            "valid_entries": valid_count,
            "invalid_entries": total_entries - valid_count,
            "pollution_count": self.pollution_count,
            "validity_ratio": valid_count / max(total_entries, 1),
        }
    
    def reset(self):
        """é‡ç½®"""
        self.kv_validity.clear()
        self.pollution_count = 0


class AttentionSinkProtector:
    """
    Attention Sinkä¿æŠ¤å™¨
    
    æ•´åˆæ‰€æœ‰ä¿æŠ¤æœºåˆ¶
    """
    
    def __init__(
        self,
        num_layers: int,
        max_seq_len: int = 4096,
        anchor_interval: int = 4,
        num_sink_tokens: int = 4,
    ):
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        
        # å­ç»„ä»¶
        self.sink_detector = AttentionSinkDetector(num_sink_tokens=num_sink_tokens)
        self.anchor_manager = AnchorLayerManager(num_layers, anchor_interval)
        self.dynamic_mask = DynamicAttentionMask(num_layers, max_seq_len)
        
        # Tokenä¿æŠ¤çŠ¶æ€
        self.token_protections: Dict[int, TokenProtection] = {}
    
    def initialize(
        self,
        input_ids: torch.Tensor,
        tokenizer: Any = None,
    ):
        """åˆå§‹åŒ–ä¿æŠ¤çŠ¶æ€"""
        # æ£€æµ‹Sinks
        protections = self.sink_detector.detect_sinks(input_ids, tokenizer)
        
        for p in protections:
            self.token_protections[p.position] = p
        
        logger.debug(f"Initialized protection for {len(protections)} tokens")
    
    def should_compute_layer(
        self,
        layer_idx: int,
        position: int,
        confidence: float = 0.0,
    ) -> Tuple[bool, str]:
        """
        å†³å®šæ˜¯å¦åº”è¯¥è®¡ç®—è¯¥å±‚
        
        Returns:
            (should_compute, reason)
        """
        # é”šç‚¹å±‚ï¼šå¼ºåˆ¶è®¡ç®—
        if self.anchor_manager.is_anchor(layer_idx):
            return True, "Anchor layer"
        
        # Sink Tokenï¼šå¼ºåˆ¶è®¡ç®—
        if self.sink_detector.is_sink(position):
            return True, "Attention Sink token"
        
        # å—ä¿æŠ¤Token
        if position in self.token_protections:
            prot = self.token_protections[position]
            if prot.level in [ProtectionLevel.SINK, ProtectionLevel.CRITICAL]:
                return True, prot.reason
        
        # å¦åˆ™ç”±SEDACå†³å®š
        return False, "SEDAC decision"
    
    def on_layer_computed(
        self,
        layer_idx: int,
        positions: torch.Tensor,
    ):
        """å±‚è®¡ç®—å®Œæˆå›è°ƒ"""
        self.dynamic_mask.mark_valid(layer_idx, positions)
        
        # æ›´æ–°Tokenä¿æŠ¤çŠ¶æ€
        for pos in positions.tolist():
            if pos in self.token_protections:
                self.token_protections[pos].layers_computed.add(layer_idx)
    
    def on_layer_skipped(
        self,
        layer_idx: int,
        positions: torch.Tensor,
    ):
        """å±‚è·³è¿‡å›è°ƒ"""
        self.dynamic_mask.mark_invalid(layer_idx, positions)
    
    def get_attention_mask(
        self,
        layer_idx: int,
        seq_len: int,
        device: torch.device,
    ) -> torch.Tensor:
        """è·å–è¯¥å±‚çš„Attention Mask"""
        return self.dynamic_mask.get_attention_mask(layer_idx, seq_len, device)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡"""
        return {
            "num_anchor_layers": len(self.anchor_manager.anchor_layers),
            "num_sink_tokens": len(self.sink_detector.sink_positions),
            "protected_tokens": len(self.token_protections),
            "mask_stats": self.dynamic_mask.get_statistics(),
        }
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self.token_protections.clear()
        self.dynamic_mask.reset()


def create_attention_sink_protector(
    num_layers: int = 32,
    anchor_interval: int = 4,
    num_sink_tokens: int = 4,
) -> AttentionSinkProtector:
    """åˆ›å»ºAttention Sinkä¿æŠ¤å™¨"""
    return AttentionSinkProtector(
        num_layers=num_layers,
        anchor_interval=anchor_interval,
        num_sink_tokens=num_sink_tokens,
    )


def demo_attention_sinks():
    """æ¼”ç¤ºAttention Sinksä¿æŠ¤"""
    print("=" * 60)
    print("Attention Sinks Demo: å…³é”®Tokenä¿æŠ¤æœºåˆ¶")
    print("=" * 60)
    
    # é…ç½®
    num_layers = 32
    seq_len = 128
    anchor_interval = 4
    num_sink_tokens = 4
    
    # åˆ›å»ºä¿æŠ¤å™¨
    protector = create_attention_sink_protector(
        num_layers=num_layers,
        anchor_interval=anchor_interval,
        num_sink_tokens=num_sink_tokens,
    )
    
    # æ¨¡æ‹Ÿè¾“å…¥
    input_ids = torch.randint(0, 32000, (1, seq_len))
    protector.initialize(input_ids)
    
    print(f"\né…ç½®:")
    print(f"  æ€»å±‚æ•°: {num_layers}")
    print(f"  é”šç‚¹é—´éš”: {anchor_interval}")
    print(f"  Sink Tokenæ•°: {num_sink_tokens}")
    
    # æ˜¾ç¤ºé”šç‚¹å±‚
    anchor_layers = sorted(protector.anchor_manager.anchor_layers)
    print(f"\né”šç‚¹å±‚ ({len(anchor_layers)}ä¸ª):")
    print(f"  {anchor_layers}")
    
    # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
    print(f"\næ¨¡æ‹Ÿæ¨ç†:")
    device = input_ids.device
    
    computed_count = 0
    skipped_count = 0
    
    for layer_idx in range(num_layers):
        # æ£€æŸ¥å‡ ä¸ªå…³é”®ä½ç½®
        test_positions = [0, 1, 2, 3, 10, 50, 100]
        
        for pos in test_positions:
            if pos >= seq_len:
                continue
            
            should_compute, reason = protector.should_compute_layer(layer_idx, pos)
            
            if should_compute:
                computed_count += 1
                protector.on_layer_computed(layer_idx, torch.tensor([pos]))
            else:
                skipped_count += 1
                protector.on_layer_skipped(layer_idx, torch.tensor([pos]))
        
        if layer_idx < 3 or layer_idx >= num_layers - 2:
            is_anchor = "ğŸ”’" if protector.anchor_manager.is_anchor(layer_idx) else "  "
            print(f"  Layer {layer_idx:2d} {is_anchor}")
        elif layer_idx == 3:
            print(f"  ...")
    
    # ç»Ÿè®¡
    stats = protector.get_statistics()
    print(f"\nç»Ÿè®¡:")
    print(f"  é”šç‚¹å±‚æ•°: {stats['num_anchor_layers']}")
    print(f"  Sink Token: {stats['num_sink_tokens']}")
    print(f"  å—ä¿æŠ¤Token: {stats['protected_tokens']}")
    print(f"  KVæœ‰æ•ˆç‡: {stats['mask_stats']['validity_ratio']*100:.1f}%")
    
    # æ˜¾ç¤ºAttention Maskç¤ºä¾‹
    print(f"\nåŠ¨æ€Attention Maskç¤ºä¾‹ (Layer 5, seq_len=8):")
    mask = protector.get_attention_mask(5, 8, device)
    print(mask.int())
    
    print("\n" + "=" * 60)
    print("Attention Sinks: ä¿æŠ¤å…³é”®Tokenï¼Œé˜²æ­¢é•¿æ–‡æœ¬å´©æºƒ")
    print("=" * 60)


if __name__ == "__main__":
    demo_attention_sinks()
