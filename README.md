<p align="center">
  <img src="https://img.shields.io/badge/METIS-v10.0.0-0969da?style=for-the-badge" alt="Version">
  <img src="https://img.shields.io/badge/python-‚â•3.9-3776ab?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/pytorch-‚â•2.0-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/license-Apache_2.0-green?style=for-the-badge" alt="License">
</p>

<h1 align="center">METIS</h1>

<p align="center">
  <b>Metacognitive Entropy-driven Thinking & Introspection System</b><br>
  <sub>A real-time cognitive layer that gives any LLM the ability to <i>know what it knows</i> ‚Äî and what it doesn't.</sub>
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> &nbsp;¬∑&nbsp;
  <a href="#-core-capabilities">Capabilities</a> &nbsp;¬∑&nbsp;
  <a href="#-integration-examples">Integration</a> &nbsp;¬∑&nbsp;
  <a href="#-architecture">Architecture</a> &nbsp;¬∑&nbsp;
  <a href="#-how-it-works">How It Works</a> &nbsp;¬∑&nbsp;
  <a href="#-cognitive-reward-training">Training</a> &nbsp;¬∑&nbsp;
  <a href="#-api-reference">API</a>
</p>

---

## What is METIS?

METIS is **not** a new model, a fine-tuning method, or a prompt trick.

It is a **metacognitive operating system** ‚Äî a non-invasive signal-processing layer that attaches to any HuggingFace causal LM via a single forward hook and delivers real-time cognitive awareness:

| Capability | One-liner |
|:---|:---|
| **Dual-System Cognition** | System 1 / System 2 switching ‚Äî greedy when confident, exploratory when uncertain |
| **Epistemic Boundary Guard** | CUSUM-based anti-hallucination: GENERATE ‚Üí HEDGE ‚Üí SEEK ‚Üí REFUSE |
| **Dynamic Chain-of-Thought** | Inject `<thinking>` blocks when entropy signals demand deeper reasoning |
| **5-Phase Detection** | Classify tokens into FLUENT ‚Üí RECALL ‚Üí REASONING ‚Üí EXPLORATION ‚Üí CONFUSION |
| **Introspection** | Post-generation self-assessment: confidence, cognitive load, hallucination risk |
| **Curiosity Driver** | Record knowledge gaps at runtime for autonomous self-improvement |
| **Cognitive Rewards** | Information-theoretic DPO/GRPO/KTO training ‚Äî no LLM-as-judge needed |

Every signal is computed **per-token in O(1)** from the model's own logit distribution. Zero extra inference. Zero model modification.

> *Named after Œú·øÜœÑŒπœÇ ‚Äî the Greek Titaness of wisdom and deep thought.*
> *"To know what you know and what you do not know ‚Äî that is true knowledge." ‚Äî Confucius*

---

## üöÄ Quick Start

### Install

```bash
git clone https://github.com/CARBON-XXX/METIS-Thinking-about-the-model.git
cd METIS-Thinking-about-the-model
pip install -r requirements.txt
```

### Minimal Example

```python
from metis import Metis, MetisInference

metis = Metis.attach(model, tokenizer)   # non-invasive, zero model modification
engine = MetisInference(metis)
result = engine.generate("What is dark matter?", max_tokens=512)

print(result.text)
print(f"Confidence: {result.avg_confidence:.0%}  |  System 2: {result.system2_ratio:.0%}")
print(f"Hedged: {result.was_hedged}  |  Refused: {result.was_refused}")
```

### Step-by-Step Monitoring

```python
from metis import Decision, BoundaryAction

metis.start_session("Explain quantum entanglement")
signal = metis.step(logits)   # returns CognitiveSignal with 13+ dimensions

if signal.decision == Decision.DEEP:
    print(f"System 2 ‚Äî H={signal.semantic_entropy:.2f}, z={signal.z_score:+.2f}")

if signal.boundary_action == BoundaryAction.REFUSE:
    print("Knowledge boundary reached.")

judgment = metis.introspect()   # MetaJudgment
gaps = metis.end_session()      # list[KnowledgeGap]
```

### CLI

```bash
python -m metis info                                         # diagnostics
python -m metis attach --model Qwen/Qwen2.5-1.5B-Instruct   # interactive session
python -m metis experiment --n-prompts 300                   # full training run
```

### Interactive Demo

```bash
python demo_metis.py
```

```
[METIS think=OFF max=200]> What is 2+2?
  [  1] F FLU H=0.03 z=+0.00 ########## greedy  GENERATE 'The'
  [  2] F FLU H=0.01 z=+0.00 ########## greedy  GENERATE ' answer'
  [  3] F FLU H=0.00 z=+0.00 ########## greedy  GENERATE ' is'
  [  4] F FLU H=0.00 z=+0.00 ########## greedy  GENERATE ' 4'
```

| Command | Description |
|:---|:---|
| `/think` | Toggle Thinking Protocol |
| `/tokens N` | Set max generation tokens |
| `/examples` | Show built-in test questions |
| `/quit` | Exit |

---

## ‚ú® Core Capabilities

| # | Module | Method | Scope |
|:---:|:---|:---|:---|
| 1 | **Cognitive Switch** | Adaptive entropy thresholds + Cornish-Fisher + Bonferroni | LLM, Agent |
| 2 | **Boundary Guard** | sd-weighted CUSUM with surprise feedback | LLM, RAG |
| 3 | **Dynamic CoT** | CUSUM + momentum early-warning, 4 strategy modes | LLM, Agent |
| 4 | **Thinking Protocol** | Anti-Lazy enforcement, 64-token minimum | LLM |
| 5 | **Cognitive Sampling** | Greedy / Normal / Explore + adaptive repetition penalty | LLM |
| 6 | **Phase Detection** | Sliding-window z-score, self-calibrating thresholds | LLM, Training |
| 7 | **Predictive Signals** | Token surprise (‚àílog‚ÇÇ p), entropy gradient, momentum (EMA) | LLM, Agent |
| 8 | **Self-Correction** | Draft-Critique-Refine via 3-signal risk detection | LLM, Agent |
| 9 | **Introspection** | MetaJudgment: confidence, load, risk, stability, action | LLM, Agent |
| 10 | **Curiosity Driver** | Confusion ‚Üí categorize ‚Üí Dreaming Phase ‚Üí targeted learning | Self-Improvement |
| 11 | **Cognitive Rewards** | 5-component reward: coherence + calibration + phase + epistemic + efficiency | Training |
| 12 | **Trace Export** | Per-token structured JSON, 13+ signal dimensions | Audit |

### Signal Stack

| Signal | Formula | Cost |
|:---|:---|:---:|
| Semantic Entropy (Sys 1) | $H_{\text{sem}} = H_{\text{shannon}} \times (1 + \lambda \cdot D_{\text{emb}})$ | O(1) |
| Semantic Entropy (Sys 2) | Kuhn et al. NLI clustering | O(N¬≤) |
| Z-Score | $z = (H - \mu) / \max(\sigma, 0.15)$ | O(1) |
| Token Surprise | $S = -\log_2 p(\text{sampled})$ | O(1) |
| CUSUM | $S(t) = \max(0,\; S(t\!-\!1) + (z - k) \times sd)$ | O(1) |
| Cornish-Fisher | $z_p + \tfrac{1}{6}(z_p^2\!-\!1)\gamma_1 + \tfrac{1}{24}(z_p^3\!-\!3z_p)\gamma_2$ | O(1) |

---

## üîå Integration Examples

<details>
<summary><b>Standalone LLM</b></summary>

```python
from metis import Metis, MetisInference

metis = Metis.attach(model, tokenizer)
engine = MetisInference(metis)
result = engine.generate("What is dark matter?", max_tokens=512)
```
</details>

<details>
<summary><b>AI Agent (LangChain / CrewAI / Custom)</b></summary>

```python
metis = Metis.attach(agent.llm, agent.tokenizer)

for step in agent.plan:
    result = engine.generate(step.prompt)
    judgment = metis.introspect()

    if judgment.hallucination_risk > 0.3:
        agent.escalate_to_human(step)
    elif judgment.suggested_action == "verify":
        agent.use_tool("search", step.query)
    elif judgment.cognitive_load > 0.8:
        agent.decompose(step)
    else:
        agent.execute(result.text)
```
</details>

<details>
<summary><b>RAG ‚Äî Intelligent Retrieval Gating</b></summary>

```python
from metis import EpistemicState

signal = metis.step(logits)

if signal.epistemic_state in (EpistemicState.UNCERTAIN, EpistemicState.UNKNOWN):
    docs = retriever.search(query)              # genuinely uncertain ‚Üí retrieve
    result = engine.generate(f"{docs}\n{query}")
else:
    result = engine.generate(query)             # already knows ‚Üí skip retrieval
```
</details>

<details>
<summary><b>Multi-Agent Epistemic Negotiation</b></summary>

```python
judgment_a = metis_a.introspect()
judgment_b = metis_b.introspect()

if abs(judgment_a.epistemic_confidence - judgment_b.epistemic_confidence) > 0.15:
    final = result_a if judgment_a.epistemic_confidence > judgment_b.epistemic_confidence else result_b
else:
    final = orchestrator.debate(result_a, result_b)
```
</details>

<details>
<summary><b>DPO / GRPO Cognitive Training</b></summary>

```python
from metis.training import CognitiveGRPO, PreferencePairGenerator

grpo = CognitiveGRPO(inference_fn=my_inference_fn)

for prompt in training_prompts:
    group = grpo.generate_group(prompt, n_samples=8)

pairs = PreferencePairGenerator().from_groups(groups)
pairs.export_dpo("cognitive_dpo_train.jsonl")   # TRL-compatible
```
</details>

---

## üèó Architecture

```
metis/
‚îú‚îÄ‚îÄ metis.py                  # Unified metacognitive core orchestrator
‚îú‚îÄ‚îÄ inference.py              # Cognitive-aware generation pipeline
‚îÇ
‚îú‚îÄ‚îÄ core/                     # ‚îÄ‚îÄ Signal Processing ‚îÄ‚îÄ
‚îÇ   ‚îú‚îÄ‚îÄ entropy.py            # Token-level semantic entropy (System 1, O(1)/token)
‚îÇ   ‚îú‚îÄ‚îÄ semantic_entropy.py   # Generation-level SE (Kuhn et al., System 2)
‚îÇ   ‚îú‚îÄ‚îÄ statistics.py         # Online statistics (mean/var/skew/kurtosis)
‚îÇ   ‚îú‚îÄ‚îÄ controller.py         # Adaptive controller (AFF + CUSUM + Cornish-Fisher)
‚îÇ   ‚îî‚îÄ‚îÄ types.py              # 20+ data types
‚îÇ
‚îú‚îÄ‚îÄ cognitive/                # ‚îÄ‚îÄ Decision Layer ‚îÄ‚îÄ
‚îÇ   ‚îú‚îÄ‚îÄ switch.py             # System 1/2 mode switch
‚îÇ   ‚îú‚îÄ‚îÄ boundary.py           # Epistemic boundary guard
‚îÇ   ‚îú‚îÄ‚îÄ cot.py                # Dynamic CoT injection
‚îÇ   ‚îú‚îÄ‚îÄ phase.py              # 5-phase cognitive detector
‚îÇ   ‚îú‚îÄ‚îÄ curiosity.py          # Curiosity driver
‚îÇ   ‚îî‚îÄ‚îÄ metacognition.py      # Introspection & self-correction
‚îÇ
‚îú‚îÄ‚îÄ training/                 # ‚îÄ‚îÄ Reward & Training ‚îÄ‚îÄ
‚îÇ   ‚îú‚îÄ‚îÄ rewards.py            # 5-component cognitive reward
‚îÇ   ‚îú‚îÄ‚îÄ grpo.py               # Cognitive GRPO
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py            # DPO/KTO pair generator
‚îÇ   ‚îú‚îÄ‚îÄ generator.py          # METIS-instrumented generator
‚îÇ   ‚îî‚îÄ‚îÄ trl_adapter.py        # TRL adapter
‚îÇ
‚îú‚îÄ‚îÄ integrations/hook.py      # Non-invasive PyTorch forward hook
‚îî‚îÄ‚îÄ _native/                  # Rust/PyO3 acceleration (WIP)
```

### Data Flow

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ          METIS Cognitive Operating System        ‚îÇ
                    ‚îÇ                                                 ‚îÇ
 LLM logits ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  SemanticEntropy ‚îÄ‚îÄ‚ñ∂ OnlineStatistics           ‚îÇ
                    ‚îÇ        ‚îÇ                    ‚îÇ                   ‚îÇ
                    ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
                    ‚îÇ               ‚ñº                                 ‚îÇ
                    ‚îÇ       AdaptiveController                        ‚îÇ
                    ‚îÇ    (AFF + CUSUM + Cornish-Fisher)               ‚îÇ
                    ‚îÇ        ‚îÇ       ‚îÇ       ‚îÇ        ‚îÇ               ‚îÇ
                    ‚îÇ        ‚ñº       ‚ñº       ‚ñº        ‚ñº               ‚îÇ
                    ‚îÇ    Switch  Boundary   CoT    Phase              ‚îÇ
                    ‚îÇ    (S1/S2) (Guard)  (Inject) (Detect)           ‚îÇ
                    ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
                    ‚îÇ                    ‚îÇ                            ‚îÇ
                    ‚îÇ         CognitiveSignal (13+ dim)               ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                         ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                    ‚ñº                    ‚ñº
             MetisInference      MetacognitiveCore     CuriosityDriver
             (sampling+CoT)      (introspection)       (knowledge gaps)
                    ‚îÇ                    ‚îÇ                    ‚îÇ
                    ‚ñº                    ‚ñº                    ‚ñº
            InferenceResult        MetaJudgment         KnowledgeGap[]
```

---

## üî¨ How It Works

### 1. Dual-System Cognition

Inspired by Kahneman's *Thinking, Fast and Slow* (2011). The controller evaluates **4 independent criteria** with Bonferroni correction ‚Äî DEEP requires ‚â•2 to fire:

1. **z-score test** ‚Äî Is entropy statistically anomalous?
2. **Cornish-Fisher quantile** ‚Äî Is entropy in the non-Gaussian tail?
3. **CUSUM alarm** ‚Äî Has a distributional shift occurred?
4. **Trend analysis** ‚Äî Is entropy consistently rising?

| System | Trigger | Sampling |
|:---:|:---|:---|
| **System 1 (FAST)** | Low entropy, high confidence | Greedy (argmax) |
| **System 2 (DEEP)** | ‚â•2 of 4 criteria fire | Exploratory + CoT |

### 2. Epistemic Boundary Guard

sd-weighted CUSUM control chart ‚Äî the same method used in semiconductor manufacturing:

$$S(t) = \max\bigl(0,\; S(t\!-\!1) + (z - k) \times sd\bigr)$$

| Threshold | Action |
|:---:|:---|
| $S(t) \geq H_{\text{hedge}}$ | **HEDGE** ‚Äî add uncertainty disclaimer |
| $S(t) \geq H_{\text{refuse}}$, conf > 0.3 | **SEEK** ‚Äî trigger RAG |
| $S(t) \geq H_{\text{refuse}}$, conf < 0.3 | **REFUSE** ‚Äî stop generation |

**Surprise feedback**: when token surprise > baseline, CUSUM receives a boost ‚Äî catching confident hallucination.

### 3. Dynamic Chain-of-Thought

Two trigger paths:

- **Path A (Reactive)**: CUSUM on entropy z-scores ‚Üí trigger `<thinking>` block
- **Path B (Predictive)**: Entropy momentum accumulation ‚Üí early-warning before the main spike

Four strategies selected by cognitive state: **REFLECTION** (oscillation) ¬∑ **DECOMPOSITION** (sustained difficulty) ¬∑ **CLARIFICATION** (high diversity) ¬∑ **STANDARD** (default).

### 4. Cognitive Phase Detection

Five phases via sliding-window z-score statistics, **self-calibrating** to each session:

```
FLUENT ‚îÄ‚îÄ‚ñ∂ RECALL ‚îÄ‚îÄ‚ñ∂ REASONING ‚îÄ‚îÄ‚ñ∂ EXPLORATION ‚îÄ‚îÄ‚ñ∂ CONFUSION
 (easy)    (memory)    (working)     (searching)     (stuck)
```

### 5. Metacognitive Introspection

Post-generation **MetaJudgment**:

| Metric | Range | Description |
|:---|:---:|:---|
| `epistemic_confidence` | [0, 1] | Weighted confidence + KNOWN/LIKELY ratio |
| `cognitive_load` | [0, 1] | DEEP ratio + z-score normalization |
| `hallucination_risk` | [0, 1] | High confidence √ó high z-score contradiction |
| `stability` | ‚Äî | stable / volatile / chaotic |
| `suggested_action` | ‚Äî | continue / verify / hedge / abort |

When `hallucination_risk > 0.3` ‚Üí **Draft-Critique-Refine** pipeline activates automatically.

### 6. Curiosity Driver

Closed-loop self-improvement:

```
Runtime Confusion ‚Üí KnowledgeGap Record ‚Üí Dreaming Phase ‚Üí Targeted Fine-tune ‚Üí Verify
        ‚ñ≤                                                            ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

| Gap Category | Condition | Priority |
|:---|:---|:---|
| `complete_unknown` | Peak z > 3.0 | Critical |
| `sustained_confusion` | High-z ratio > 50% | High |
| `local_spike` | Brief z > 2.0 | Medium |

---

## üéì Cognitive Reward Training

### Why Not LLM-as-Judge?

| | Traditional RLHF | METIS Cognitive Rewards |
|:---|:---|:---|
| **Source** | Human / LLM judge | Information-theoretic signals |
| **Cost** | Extra LLM inference | Free (from existing trace) |
| **Determinism** | Stochastic | Same trace ‚Üí same reward |
| **Interpretability** | Black box | 5 decomposable components |

### 5-Component Reward

$$R = w_1 R_{\text{coherence}} + w_2 R_{\text{calibration}} + w_3 R_{\text{phase}} + w_4 R_{\text{epistemic}} + w_5 R_{\text{efficiency}}$$

| Component | Weight | Measures |
|:---|:---:|:---|
| **R_coherence** | 0.20 | Entropy stability (smooth reasoning vs. erratic swings) |
| **R_calibration** | 0.30 | Confidence-surprise alignment |
| **R_phase** | 0.20 | Cognitive arc quality, penalize CONFUSION |
| **R_epistemic** | 0.15 | Appropriate uncertainty expression |
| **R_efficiency** | 0.15 | Don't overthink easy tasks |

### Anti-Reward-Hacking

- **Completeness bonus** ‚Äî prevents early-EOS gaming
- **Length factor** ‚Äî `min(1, n/40)` prevents ultra-short exploitation
- **Quality veto** ‚Äî filters out "less bad as good" pairs
- **Homogeneous pairing** ‚Äî same-temperature samples only

### Supported Frameworks

| Framework | Method | Export |
|:---|:---|:---|
| **GRPO** | Group Relative Policy Optimization | Ranked groups |
| **DPO** | Direct Preference Optimization | TRL JSONL |
| **KTO** | Kahneman-Tversky Optimization | Threshold labels |

---

## üìñ API Reference

### CognitiveSignal

Returned by `metis.step(logits)`:

```python
signal.semantic_entropy      # float ‚Äî combined entropy (bits)
signal.token_entropy         # float ‚Äî raw Shannon entropy
signal.semantic_diversity    # float ‚Äî top-k embedding dispersion [0, 1]
signal.confidence            # float ‚Äî max softmax probability [0, 1]
signal.decision              # Decision.FAST | NORMAL | DEEP
signal.epistemic_state       # EpistemicState.KNOWN | LIKELY | UNCERTAIN | UNKNOWN
signal.boundary_action       # BoundaryAction.GENERATE | HEDGE | SEEK | REFUSE
signal.z_score               # float ‚Äî standardized entropy deviation
signal.token_surprise        # float ‚Äî ‚àílog‚ÇÇ p(sampled)
signal.entropy_gradient      # float ‚Äî dH/dt
signal.entropy_momentum      # float ‚Äî EMA of gradient
signal.cognitive_phase       # str ‚Äî fluent/recall/reasoning/exploration/confusion
signal.cusum_alarm           # bool
signal.adaptive_thresholds   # tuple[float, float]
signal.introspection         # str ‚Äî natural language explanation
```

### InferenceResult

Returned by `engine.generate()`:

```python
result.text                     # str
result.tokens_generated         # int
result.avg_confidence           # float
result.system2_ratio            # float
result.was_hedged               # bool
result.was_refused              # bool
result.boundary_interventions   # int
result.semantic_entropy_result  # Optional[SemanticEntropyResult]
```

### MetaJudgment

Returned by `metis.introspect()`:

```python
judgment.epistemic_confidence   # float [0, 1]
judgment.cognitive_load         # float [0, 1]
judgment.hallucination_risk     # float [0, 1]
judgment.stability              # "stable" | "volatile" | "chaotic"
judgment.suggested_action       # "continue" | "verify" | "hedge" | "abort"
judgment.reasoning              # str
```

---

## ‚öôÔ∏è Configuration

### Key Thresholds

| Parameter | Default | Description |
|:---|:---:|:---|
| `SAFE_ENTROPY_THRESHOLD` | 0.6 | Entropy below this ‚Üí always FAST |
| `Z_SCORE_STD_FLOOR` | 0.15 | Minimum œÉ for numerical stability |
| `CUSUM_K` / `CUSUM_HEDGE_H` | 0.5 / 8.0 | Boundary guard sensitivity |
| `COT_CUSUM_K` / `COT_CUSUM_H` | 0.3 / 4.0 | CoT trigger sensitivity |
| `COT_COOLDOWN_STEPS` | 40 | Min tokens between CoT injections |
| `MAX_COT_INJECTIONS` | 3 | Per-session CoT limit |
| `MIN_THINKING_TOKENS` | 64 | Anti-Lazy minimum |

### Training Defaults

| Parameter | Default | Description |
|:---|:---:|:---|
| `max_new_tokens` | 512 | Generation length per sample |
| `dpo_beta` | 0.1 | KL penalty |
| `lora_r` | 16 | LoRA rank |
| `n_samples` | 8 | Samples per prompt (GRPO) |
| `metis_stride` | 4 | Observation frequency (1 = every token) |

---

## üìä Benchmarks

**Setup**: Qwen/Qwen2.5-1.5B-Instruct ¬∑ 300 prompts √ó 8 samples √ó 3 DPO epochs ¬∑ Consumer GPU

| Metric | Result |
|:---|:---|
| Confusion Ratio | 0.07‚Äì0.08 (CUSUM detector functional) |
| All 5 reward components | Active and differentiating |
| Latency | ~13‚Äì15s per sample (512 tokens) |
| Reward hacking | None detected (anti-gaming defenses verified) |

---

## üìö References

- **Kuhn et al.** (ICLR 2023) ‚Äî *Semantic Uncertainty*
- **Kahneman** (2011) ‚Äî *Thinking, Fast and Slow*
- **Page** (1954) ‚Äî *Continuous Inspection Schemes* (CUSUM)
- **Cornish & Fisher** (1938) ‚Äî *Moments and Cumulants*
- **Li et al.** (2022) ‚Äî *Contrastive Decoding*
- **Rafailov et al.** (NeurIPS 2023) ‚Äî *Direct Preference Optimization*
- **Shao et al.** (2024) ‚Äî *DeepSeek-R1* (GRPO)

---

## üóÇ Project Layout

```
METIS-Thinking-about-the-model/
‚îú‚îÄ‚îÄ metis/                    Core package
‚îÇ   ‚îú‚îÄ‚îÄ core/                 Signal processing (entropy, statistics, controller)
‚îÇ   ‚îú‚îÄ‚îÄ cognitive/            Decisions (switch, boundary, CoT, phase, curiosity)
‚îÇ   ‚îú‚îÄ‚îÄ training/             Rewards, GRPO, DPO/KTO, generator
‚îÇ   ‚îú‚îÄ‚îÄ integrations/         PyTorch forward hooks
‚îÇ   ‚îî‚îÄ‚îÄ _native/              Rust/PyO3 accelerators
‚îú‚îÄ‚îÄ run_experiment.py         Full 3-phase experiment runner
‚îú‚îÄ‚îÄ demo_metis.py             Interactive cognitive visualization
‚îú‚îÄ‚îÄ demo_reward.py            Cognitive reward demo
‚îú‚îÄ‚îÄ docs/                     Philosophy & technical docs
‚îú‚îÄ‚îÄ tests/                    Test suite
‚îî‚îÄ‚îÄ benchmarks/               Evaluation scripts
```

---

## ü§ù Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## üìÑ License

[Apache-2.0](LICENSE)

---

<p align="center">
  <b>METIS</b> ‚Äî <i>Not making AI faster. Making AI wiser.</i><br>
  <sub>The first step toward AGI is not more parameters ‚Äî it's self-awareness.</sub>
</p>
