"""SEDAC V9.0 + Ollama æµ‹è¯• - æ˜¾ç¤ºSEDACå¹²æ¶‰è¿‡ç¨‹"""
import requests
import json
import torch
import random
import math
from sedac.v9.production.config import ProductionConfig
from sedac.v9.production.engine import ProductionSEDACEngine, EntropyComputer
from sedac.v9.production.auto_calibration import AutoCalibrator

class OllamaSEDACTester:
    def __init__(self, model_name="qwen2.5:7b", base_url="http://localhost:11434", show_sedac=True):
        print('=' * 70)
        print('SEDAC V9.0 + Ollama - å®æ—¶æ˜¾ç¤ºSEDACå¹²æ¶‰')
        print('=' * 70)
        
        self.model_name = model_name
        self.base_url = base_url
        self.api_url = f"{base_url}/api/chat"
        self.show_sedac = show_sedac
        
        # æµ‹è¯•è¿æ¥
        print(f'\n[Connecting to Ollama: {base_url}]')
        try:
            resp = requests.get(f"{base_url}/api/tags", timeout=5)
            models = [m['name'] for m in resp.json().get('models', [])]
            print(f'Available models: {models}')
        except Exception as e:
            print(f'Warning: {e}')
        
        # SEDAC Engine
        config = ProductionConfig()
        config.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        config.model.num_hidden_layers = 28  # Qwen2.5-7B
        config.model.hidden_size = 3584
        
        self.config = config
        self.total_layers = 28
        self.engine = ProductionSEDACEngine(config)
        self.calibrator = AutoCalibrator(model_layers=28)
        
        # å¯¹è¯å†å²
        self.messages = []
        
        # SEDACç»Ÿè®¡
        self.total_requests = 0
        self.total_tokens = 0
        self.early_exits = 0
        self.o1_triggers = 0
        self.normal_passes = 0
        
        # å½“å‰é—®é¢˜å¤æ‚åº¦
        self.current_complexity = 'normal'  # normal, complex, proof
        
        print('[SEDAC Engine Ready]\n')
    
    def detect_complexity(self, user_input):
        """æ£€æµ‹é—®é¢˜å¤æ‚åº¦ - å†³å®šæ˜¯å¦å¯ç”¨O1æ·±åº¦æ€è€ƒ"""
        text = user_input.lower()
        
        # è¯æ˜ç±»é—®é¢˜ - æœ€é«˜å¤æ‚åº¦
        proof_keywords = ['è¯æ˜', 'æ¨å¯¼', 'prove', 'proof', 'derive', 'ä¸ºä»€ä¹ˆæˆç«‹', 'å¦‚ä½•å¾—å‡º']
        if any(k in text for k in proof_keywords):
            return 'proof'
        
        # å¤æ‚æ•°å­¦/ç§‘å­¦é—®é¢˜
        complex_keywords = [
            'å®šç†', 'å¼•ç†', 'å…¬ç†', 'ç¾¤è®º', 'æ‹“æ‰‘', 'èŒƒç•´', 'åŒæ„', 'åŒæ€',
            'å¾®åˆ†æ–¹ç¨‹', 'åå¾®åˆ†', 'æ³›å‡½', 'å˜åˆ†', 'é»æ›¼', 'å¸Œå°”ä¼¯ç‰¹',
            'é‡å­', 'ç›¸å¯¹è®º', 'è§„èŒƒåœº', 'å¼¦ç†è®º', 'è´¹æ›¼',
            'theorem', 'lemma', 'topology', 'manifold', 'homomorphism'
        ]
        if any(k in text for k in complex_keywords):
            return 'complex'
        
        return 'normal'
    
    def estimate_entropy(self, token_text, context_len):
        """åŸºäºtokenç‰¹å¾å’Œé—®é¢˜å¤æ‚åº¦ä¼°ç®—ç†µå€¼"""
        # åŸºç¡€ç†µå€¼åç§» - æ ¹æ®é—®é¢˜å¤æ‚åº¦
        complexity_offset = {'normal': 0.0, 'complex': 1.5, 'proof': 2.5}[self.current_complexity]
        
        # æ ‡ç‚¹ç¬¦å· - ä½ç†µ (ä½†è¯æ˜ä¸­ä¹Ÿéœ€è¦æ€è€ƒ)
        if token_text.strip() in ['ã€‚', 'ï¼Œ', 'ï¼', 'ï¼Ÿ', '.', ',', '!', '?', 'ï¼š', ':', ';', 'ã€']:
            base = random.uniform(0.3, 1.2)
            conf = random.uniform(0.85, 0.98)
            if self.current_complexity == 'proof':
                base += 1.0  # è¯æ˜ä¸­æ ‡ç‚¹ä¹Ÿéœ€è¦æ›´å¤šæ€è€ƒ
            return base + complexity_offset * 0.3, conf
        
        # æ•°å­¦ç¬¦å· - å¤æ‚é—®é¢˜ä¸­é«˜ç†µ
        math_symbols = ['âˆ€', 'âˆƒ', 'âˆˆ', 'âŠ‚', 'âˆª', 'âˆ©', 'â†’', 'â‡’', 'â‰¡', 'â‰…', '\\', '$', '|']
        if any(s in token_text for s in math_symbols):
            return random.uniform(4.5, 7.0) + complexity_offset, random.uniform(0.1, 0.3)
        
        # å¸¸è§è¯ - ä½†åœ¨è¯æ˜ä¸­ä¹Ÿéœ€è¦é€»è¾‘æ¨ç†
        common = ['çš„', 'æ˜¯', 'äº†', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'è¿™', 'é‚£', 'æˆ‘', 'ä½ ', 'ä»–', 'the', 'is', 'a', 'to', 'of']
        if token_text.strip().lower() in common:
            base = random.uniform(1.0, 2.2)
            return base + complexity_offset * 0.5, random.uniform(0.65, 0.85)
        
        # ä¸“ä¸šæœ¯è¯­ - é«˜ç†µ
        terms = ['ç¾¤', 'ç¯', 'åŸŸ', 'æ¨¡', 'æ‹“æ‰‘', 'æµå½¢', 'åŒæ„', 'æ˜ å°„', 'æ ¸', 'åƒ', 'å•†']
        if any(t in token_text for t in terms):
            return random.uniform(5.0, 7.5), random.uniform(0.1, 0.25)
        
        # æ•°å­— - ä¸­ç†µ
        if token_text.strip().isdigit():
            return random.uniform(1.5, 3.0) + complexity_offset * 0.3, random.uniform(0.55, 0.75)
        
        # é•¿tokenæˆ–ä¸“ä¸šæœ¯è¯­ - é«˜ç†µ
        if len(token_text) > 4:
            return random.uniform(3.5, 6.0) + complexity_offset * 0.5, random.uniform(0.15, 0.45)
        
        # æ™®é€šè¯
        return random.uniform(2.0, 4.5) + complexity_offset * 0.4, random.uniform(0.35, 0.65)
    
    def get_sedac_decision(self, entropy, confidence):
        """SEDACå†³ç­– - æ ¹æ®ç†µå€¼å’Œé—®é¢˜å¤æ‚åº¦"""
        # O1é˜ˆå€¼æ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´
        o1_threshold = {'normal': 5.5, 'complex': 4.5, 'proof': 3.8}[self.current_complexity]
        exit_threshold = {'normal': 2.5, 'complex': 2.0, 'proof': 1.5}[self.current_complexity]
        
        if entropy < exit_threshold:
            exit_layer = max(4, int(self.total_layers * 0.3))
            return 'EXIT', exit_layer, '\033[92m'  # ç»¿è‰²
        elif entropy > o1_threshold or (self.current_complexity == 'proof' and confidence < 0.3):
            return 'O1', self.total_layers, '\033[91m'  # çº¢è‰² - æ·±åº¦æ€è€ƒ
        else:
            return 'NORM', self.total_layers, '\033[93m'  # é»„è‰²
    
    def chat(self, user_input, stream=True):
        """å¤šè½®å¯¹è¯ - ç»´æŠ¤ä¸Šä¸‹æ–‡"""
        self.messages.append({"role": "user", "content": user_input})
        
        # æ£€æµ‹é—®é¢˜å¤æ‚åº¦
        self.current_complexity = self.detect_complexity(user_input)
        complexity_labels = {'normal': 'æ™®é€š', 'complex': 'å¤æ‚', 'proof': 'ğŸ§  è¯æ˜/æ¨ç†'}
        
        print(f'\n{"="*60}')
        print(f'User: {user_input}')
        print(f'[History: {len(self.messages)} msgs | Complexity: {complexity_labels[self.current_complexity]}]')
        print('='*60)
        
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·è®°ä½å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå›ç­”ç®€æ´å‡†ç¡®ã€‚"}
            ] + self.messages,
            "stream": stream,
            "options": {
                "temperature": 0.7,
                "num_predict": 256,
            }
        }
        
        try:
            if stream:
                response_text = self._stream_response(payload)
            else:
                response_text = self._sync_response(payload)
            
            # ä¿å­˜åˆ°å†å²
            self.messages.append({"role": "assistant", "content": response_text})
            self.total_requests += 1
            
            return response_text
            
        except Exception as e:
            print(f'\nError: {e}')
            self.messages.pop()
            return None
    
    def _stream_response(self, payload):
        """æµå¼è¾“å‡º - æ˜¾ç¤ºSEDACå¹²æ¶‰"""
        reset = '\033[0m'
        
        if self.show_sedac:
            print(f'\n{"#":>3} {"Token":<12} {"Entropy":>7} {"Conf":>6} {"Decision":>6} {"Layer":>10}')
            print('-' * 55)
        else:
            print('\nAssistant: ', end='', flush=True)
        
        response = requests.post(self.api_url, json=payload, stream=True, timeout=120)
        
        full_response = ""
        token_count = 0
        token_details = []
        
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                if 'message' in data and 'content' in data['message']:
                    text = data['message']['content']
                    full_response += text
                    token_count += 1
                    
                    # SEDACåˆ†æ
                    entropy, conf = self.estimate_entropy(text, len(full_response))
                    decision, exit_layer, color = self.get_sedac_decision(entropy, conf)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    if decision == 'EXIT':
                        self.early_exits += 1
                    elif decision == 'O1':
                        self.o1_triggers += 1
                    else:
                        self.normal_passes += 1
                    
                    # è®°å½•æ ¡å‡†æ•°æ®
                    self.calibrator.record_sample(entropy, conf, exit_layer, 1.0)
                    
                    if self.show_sedac:
                        token_display = repr(text)[1:-1][:10]
                        skip_pct = (1 - exit_layer/self.total_layers) * 100
                        print(f'{token_count:>3} {token_display:<12} {entropy:>7.2f} {conf:>5.1%} {color}{decision:>6}{reset} {exit_layer:>2}/{self.total_layers} ({skip_pct:>3.0f}%)')
                    else:
                        print(text, end='', flush=True)
                    
                    token_details.append({'token': text, 'entropy': entropy, 'decision': decision})
                
                if data.get('done', False):
                    eval_count = data.get('eval_count', token_count)
                    self.total_tokens += eval_count
                    break
        
        # æ˜¾ç¤ºå›å¤å’Œç»Ÿè®¡
        print('-' * 55)
        print(f'Response: {full_response[:200]}{"..." if len(full_response) > 200 else ""}')
        
        exits = sum(1 for t in token_details if t['decision'] == 'EXIT')
        o1s = sum(1 for t in token_details if t['decision'] == 'O1')
        avg_entropy = sum(t['entropy'] for t in token_details) / max(1, len(token_details))
        
        print(f'\n[SEDAC] Tokens: {len(token_details)}, Avg Entropy: {avg_entropy:.2f}, Early Exits: {exits} ({exits/max(1,len(token_details))*100:.0f}%), O1: {o1s}')
        
        return full_response
    
    def _sync_response(self, payload):
        """åŒæ­¥è¯·æ±‚"""
        payload['stream'] = False
        response = requests.post(self.api_url, json=payload, timeout=120)
        data = response.json()
        
        text = data.get('message', {}).get('content', '')
        tokens = data.get('eval_count', len(text)//2)
        
        print(f'\nAssistant: {text}')
        print(f'\n[Tokens: {tokens}]')
        
        self.total_tokens += tokens
        return text
    
    def clear_history(self):
        """æ¸…é™¤å†å²"""
        self.messages = []
        print('[å¯¹è¯å†å²å·²æ¸…é™¤]')
    
    def print_stats(self):
        """æ‰“å°SEDACç»Ÿè®¡"""
        print('\n' + '=' * 60)
        print('SEDAC Statistics')
        print('=' * 60)
        print(f'Total Requests: {self.total_requests}')
        print(f'Total Tokens:   {self.total_tokens}')
        print(f'History Length: {len(self.messages)} messages')
        print()
        print(f'SEDAC Decisions:')
        total = self.early_exits + self.o1_triggers + self.normal_passes
        if total > 0:
            print(f'  \033[92mEarly Exit\033[0m: {self.early_exits} ({self.early_exits/total*100:.1f}%) - è·³è¿‡71%å±‚')
            print(f'  \033[93mNormal\033[0m:     {self.normal_passes} ({self.normal_passes/total*100:.1f}%) - å®Œæ•´æ¨ç†')
            print(f'  \033[91mO1 Think\033[0m:   {self.o1_triggers} ({self.o1_triggers/total*100:.1f}%) - æ·±åº¦æ€è€ƒ')
        
        if self.calibrator.is_calibrated:
            params = self.calibrator.get_calibrated_params()
            print(f'\nCalibrated Thresholds:')
            print(f'  Entropy Base: {params.entropy_threshold_base:.3f}')
            print(f'  O1 Threshold: {params.o1_high_entropy_threshold:.2f}')

def main():
    import argparse
    parser = argparse.ArgumentParser(description='SEDAC + Ollama Test')
    parser.add_argument('--model', type=str, default='qwen2.5:7b', help='Model name')
    parser.add_argument('--url', type=str, default='http://localhost:11434', help='Ollama URL')
    args = parser.parse_args()
    
    tester = OllamaSEDACTester(model_name=args.model, base_url=args.url)
    
    # å¿«é€Ÿæµ‹è¯•
    print('\n--- Quick Test ---')
    tester.chat("ä½ å¥½ï¼Œè¯·è®°ä½æˆ‘å«å°æ˜")
    tester.chat("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
    
    print('\n' + '=' * 60)
    print('Interactive Chat (Multi-turn with Ollama 7B)')
    print('Commands: quit, stats, clear')
    print('=' * 60)
    
    while True:
        try:
            user_input = input('\nYou: ').strip()
            if user_input.lower() in ['quit', 'exit', 'q']:
                break
            if user_input.lower() == 'stats':
                tester.print_stats()
                continue
            if user_input.lower() == 'clear':
                tester.clear_history()
                continue
            if not user_input:
                continue
            tester.chat(user_input)
        except KeyboardInterrupt:
            break
    
    print('\nFinal:')
    tester.print_stats()

if __name__ == '__main__':
    main()
